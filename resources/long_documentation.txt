ModelApplier
This operator applies a <i>Model</i> to an <i>ExampleSet</i>. All
 parameters of the training process should be stored within the model.
 However, this operator is able to take any parameters for the rare case that
 the model can use some parameters during application. Models can be read from
 a file by using a <i>ModelLoader</i>.
#####
ModelUpdater
This operator updates a <i>Model</i> with an <i>ExampleSet</i>. Please note
 that the model must return true for <i>Model#isUpdatable()</i> in order to be 
 usable with this operator.
#####
OperatorChain
A simple operator chain which can have an arbitrary number of inner
 operators. The operators are subsequently applied and their output is used as
 input for the succeeding operator. The input of the operator chain is used as
 input for the first inner operator and the output of the last operator is
 used as the output of the operator chain.
#####

CommandLineOperator
This operator executes a system command. The command and all its arguments
 are specified by the parameter <code>command</code>. The standard output
 stream and the error stream of the process can be redirected to the logfile. <br/>
 Please note also that the command is system dependent. Characters that have
 special meaning on the shell like e.g. the pipe symbol or brackets and braces
 do not have a special meaning to Java. <br/> The method
 <code>Runtime.exec(String)</code> is used to execute the command. Please
 note, that this (Java) method parses the string into tokens before it is
 executed. These tokens are <em>not</em> interpreted by a shell (which?). If
 the desired command involves piping, redirection or other shell features, it
 is best to create a small shell script to handle this.
#####
Experiment
Each process must contain exactly one operator of this class and it must
 be the root operator of the process. The only purpose of this operator is
 to provide some parameters that have global relevance.
#####
FileEcho
This operator simply writed the specified text into the specified file. This can
 be useful in combination with the <i>ProcessBranch</i> operator. For example,
 one could write the success or non-success of a process into the same file
 depending on the condition specified by a process branch.
#####
IOConsumer
Most RapidMiner operators should define their desired input and delivered output in
 a senseful way. In some cases operators can produce additional output which
 is indicated with a boolean parameter. Other operators are able to deliver
 their input as output instead of consuming it (parameter keep_...). However,
 in some cases it might be usefull to delete unwanted output to ensure that
 following operators use the correct input object. Furthermore, some operators
 produce additional unneeded and therefore unconsumed output. In an iterating
 operator chain this unneeded output will grow with each iteration. Therefore,
 the IOConsumeOperator can be used to delete one (the n-th) object of a given type 
 (indicated by delete_one), all input objects of a given type (indicated by 
 delete_all), all input objects but those of a given type (indicated by
 delete_all_but), or all input objects of the given type except for
 the n-th object of the type.
#####
IOMultiplier
In some cases you might want to apply different parts of the process on
 the same input object. You can use this operator to create <code>k</code>
 copies of the given input object.
#####
IOSelector
<p>This operator allows to choose special IOObjects from the given input. Bringing an IOObject to
 the front of the input queue allows the next operator to directly perform its action on the
 selected object. Please note that counting for the parameter value
 starts with one, but usually the IOObject which was added at last gets the number one, the object
 added directly before get number two and so on.</p>
 
 <p>The user can specify with the parameter delete_others what will happen to the non-selected input objects
 of the specified type: if this parameter is set to true, all other IOObjects of the specified type
 will be removed by this operator. Otherwise (default), the objects will all be kept and the selected
 objects will just be brought into front.</p>
#####
MacroDefinition
<p>(Re-)Define macros for the current process. Macros will be replaced in the value strings
 of parameters by the macro values defined in the parameter list of this operator.
 Please note that this features is basically only supported for string type parameter
 values (strings or files) and not for numerical or list types.</p>
 
 <p>In the parameter list of this operator, you have to define the macro name (without the enclosing brackets) and
 the macro value. The defined macro can then be used in all succeeding operators as parameter
 value for string type parameters. A macro must then be enclosed by &quot;MACRO_START&quot; and 
 &quot;MACRO_END&quot;.</p>
  
 <p>There are several predefined macros:</p>
 <ul>
 <li>MACRO_STARTprocess_nameMACRO_END: will be replaced by the name of the process (without path and extension)</li>
 <li>MACRO_STARTprocess_fileMACRO_END: will be replaced by the file name of the process (with extension)</li> 
 <li>MACRO_STARTprocess_pathMACRO_END: will be replaced by the complete absolute path of the process file</li>
 </ul>
 
 <p>In addition to those the user might define arbitrary other macros which will be replaced
 by arbitrary string during the process run. Please note also that several other short macros
 exist, e.g. MACRO_STARTaMACRO_END for the number of times the current operator was applied.
 Please refer to the section about macros in the RapidMiner tutorial.</p>
#####
MaterializeDataInMemory
Creates a fresh and clean copy of the data in memory. 
 Might be very useful in combination with the
 <i>MemoryCleanUp</i> operator after large preprocessing trees using
 lot of views or data copies.
#####
MemoryCleanUp
Cleans up unused memory resources. Might be very useful in combination with the
 <i>MaterializeDataInMemory</i> operator after large preprocessing trees using
 lot of views or data copies. Internally, this operator simply invokes a 
 garbage collection from the underlying Java programming language.
#####
Process
Each process must contain exactly one operator of this class and it must
 be the root operator of the process. The only purpose of this operator is
 to provide some parameters that have global relevance.
#####
SingleMacroDefinition
<p>(Re-)Define macros for the current process. Macros will be replaced in the value strings
 of parameters by the macro values defined as a parameter of this operator.
 Please note that this features is basically only supported for string type parameter
 values (strings or files) and not for numerical or list types. In contrast to the
 usual MacroDefinitionOperator, this operator only supports the definition of a single
 macro and can hence be used inside of parameter iterations.</p>
 
 <p>You have to define the macro name (without the enclosing brackets) and
 the macro value. The defined macro can then be used in all succeeding operators as parameter
 value for string type parameters. A macro must then be enclosed by &quot;MACRO_START&quot; and 
 &quot;MACRO_END&quot;.</p>
  
 <p>There are several predefined macros:</p>
 <ul>
 <li>MACRO_STARTprocess_nameMACRO_END: will be replaced by the name of the process (without path and extension)</li>
 <li>MACRO_STARTprocess_fileMACRO_END: will be replaced by the file name of the process (with extension)</li> 
 <li>MACRO_STARTprocess_pathMACRO_END: will be replaced by the complete absolute path of the process file</li>
 </ul>
 
 <p>In addition to those the user might define arbitrary other macros which will be replaced
 by arbitrary string during the process run. Please note also that several other short macros
 exist, e.g. MACRO_STARTaMACRO_END for the number of times the current operator was applied.
 Please refer to the section about macros in the RapidMiner tutorial.</p>
#####

ArffExampleSetWriter
Writes values of all examples into an ARFF file which can be used
 by the machine learning library Weka. The ARFF format is described in the 
 <i>ArffExampleSource</i> operator which is able to read ARFF files to
 make them usable with RapidMiner.
#####
ArffExampleSource
<p>This operator can read ARFF files known from the machine learning library Weka. 
 An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes 
 a list of instances sharing a set of attributes. ARFF files were developed by the 
 Machine Learning Project at the Department of Computer Science of The University 
 of Waikato for use with the Weka machine learning software.</p>
 
 <p>ARFF files have two distinct sections. The first section is the Header information, 
 which is followed the Data information. The Header of the ARFF file contains the name 
 of the relation (@RELATION, ignored by RapidMiner) and a list of the attributes, each of which
 is defined by a starting @ATTRIBUTE followed by its name and its type.</p>
 
 <p>Attribute declarations take the form of an orderd sequence of @ATTRIBUTE statements. 
 Each attribute in the data set has its own @ATTRIBUTE statement which uniquely defines 
 the name of that attribute and it's data type. The order the attributes are declared 
 indicates the column position in the data section of the file. For example, if an 
 attribute is the third one declared all that attributes values will be found in the third 
 comma delimited column.</p>
 
 <p>The possible attribute types are:</p>
 <ul>
 <li><code>numeric</code></li>
 <li><code>integer</code></li>
 <li><code>real</code></li>
 <li><code>{nominalValue1,nominalValue2,...}</code> for nominal attributes</li>
 <li><code>string</code> for nominal attributes without distinct nominal values (it is 
 however recommended to use the nominal definition above as often as possible)</li>
 <li><code>date [date-format]</code> (currently not supported by RapidMiner)</li>
 </ul>
 
 <p>Valid examples for attribute definitions are <br/>
 <code>@ATTRIBUTE petalwidth REAL</code> <br/>
 <code>@ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica}</code>
 </p>
 
 <p>The ARFF Data section of the file contains the data declaration line @DATA followed
 by the actual example data lines. Each example is represented on a single line, with 
 carriage returns denoting the end of the example. Attribute values for each example 
 are delimited by commas. They must appear in the order that they were declared in the 
 header section (i.e. the data corresponding to the n-th @ATTRIBUTE declaration is 
 always the n-th field of the example line). Missing values are represented by a single 
 question mark, as in:<br/>
 <code>4.4,?,1.5,?,Iris-setosa</code></p>
 
 <p>A percent sign (%) introduces a comment and will be ignored during reading. Attribute
 names or example values containing spaces must be quoted with single quotes ('). Please
 note that the sparse ARFF format is currently only supported for numerical attributes. 
 Please use one of the other options for sparse data files provided by RapidMiner if you also 
 need sparse data files for nominal attributes.</p>
 
 <p>Please have a look at the Iris example ARFF file provided in the data subdirectory 
 of the sample directory of RapidMiner to get an idea of the described data format.</p>
#####
AttributeConstructionsLoader
Loads an attribute set from a file and constructs the desired features. If
 keep_all is false, original attributes are deleted before the new ones are
 created. This also means that a feature selection is performed if only a
 subset of the original features was given in the file.
#####
AttributeConstructionsWriter
Writes all attributes of an example set to a file. Each line holds the
 construction description of one attribute. This file can be read in another
 process using the
 <i>FeatureGenerationOperator</i> or
 <i>AttributeConstructionsLoader</i>.
#####
AttributeWeightsLoader
Reads the weights for all attributes of an example set from a file and
 creates a new <i>AttributeWeights</i> IOObject. This object can be used for
 scaling the values of an example set with help of the
 <i>AttributeWeightsApplier</i> operator.
#####
AttributeWeightsWriter
Writes the weights of all attributes of an example set to a file. Therefore a
 <i>AttributeWeights</i> object is needed in the input of this operator. Each
 line holds the name of one attribute and its weight. This file can be read in
 another process using the <i>AttributeWeightsLoader</i> and the
 <i>AttributeWeightsApplier</i>.
#####
BibtexExampleSource
This operator can read BibTeX files. It uses Stefan Haustein's kdb tools.
#####
C45ExampleSource
<p>Loads data given in C4.5 format (names and data file). Both files must be in
 the same directory. You can specify one of the C4.5 files (either the data or
 the names file) or only the filestem.</p>
 
 <p>For a dataset named "foo", you will have two files: foo.data and foo.names. 
 The .names file describes the dataset, while the .data file contains the examples 
 which make up the dataset.</p>

 <p>The files contain series of identifiers and numbers with some surrounding 
 syntax. A | (vertical bar) means that the remainder of the line should be 
 ignored as a comment. Each identifier consists of a string of characters that 
 does not include comma, question mark or colon. Embedded whitespce is also permitted 
 but multiple whitespace is replaced by a single space.</p>
 
 <p>The .names file contains a series of entries that describe the classes, 
 attributes and values of the dataset.  Each entry can be terminated with a period, 
 but the period can be omited if it would have been the last thing on a line.  
 The first entry in the file lists the names of the classes, separated by commas. 
 Each successive line then defines an attribute, in the order in which they will appear 
 in the .data file, with the following format:</p>

 <pre>
   attribute-name : attribute-type
 </pre>

 <p>
 The attribute-name is an identifier as above, followed by a colon, then the attribute 
 type which must be one of</p>
 
 <ul>
 <li><code>continuous</code> If the attribute has a continuous value.</li>
 <li><code>discrete [n]</code> The word 'discrete' followed by an integer which 
     indicates how many values the attribute can take (not recommended, please use the method
     depicted below for defining nominal attributes).</li> 
 <li><code>[list of identifiers]</code> This is a discrete, i.e. nominal, attribute with the 
     values enumerated (this is the prefered method for discrete attributes). The identifiers 
     should be separated by commas.</li>
 <li><code>ignore</code> This means that the attribute should be ignored - it won't be used.
     This is not supported by RapidMiner, please use one of the attribute selection operators after
     loading if you want to ignore attributes and remove them from the loaded example set.</li>
 </ul>
 
 <p>Here is an example .names file:</p>
 <pre>
   good, bad.
   dur: continuous.
   wage1: continuous.
   wage2: continuous.
   wage3: continuous.
   cola: tc, none, tcf.
   hours: continuous.
   pension: empl_contr, ret_allw, none.
   stby_pay: continuous.
   shift_diff: continuous.
   educ_allw: yes, no.
   ...
 </pre>

 <p>Foo.data contains the training examples in the following format: one example per line, 
 attribute values separated by commas, class last, missing values represented by "?". 
 For example:</p>

 <pre>
   2,5.0,4.0,?,none,37,?,?,5,no,11,below_average,yes,full,yes,full,good
   3,2.0,2.5,?,?,35,none,?,?,?,10,average,?,?,yes,full,bad
   3,4.5,4.5,5.0,none,40,?,?,?,no,11,average,?,half,?,?,good
   3,3.0,2.0,2.5,tc,40,none,?,5,no,10,below_average,yes,half,yes,full,bad
   ...
 </pre>
#####
CSVExampleSetWriter
<p>This operator can be used to write data into CSV files (Comma Separated Values). 
 The values and columns are separated by &quot;;&quot;. Missing data values are 
 indicated by empty cells.</p>
#####
CSVExampleSource
<p>This operator can read csv files. All values must be separated by
 &quot;,&quot;, by &quot;;&quot;, or by white space like tabs. 
 The first line is used for attribute names as default.</p> 
 
 <p>For other file formats or column separators you can
 use in almost all cases the operator <i>SimpleExampleSource</i>
 or, if this is not sufficient, the operator <i>ExampleSource</i>.</p>
#####
ChurnReductionExampleSetGenerator
Generates a random example set for testing purposes. The data represents a direct mailing
 example set.
#####
ClusterModelReader
Reads a single cluster model from a file.
#####
ClusterModelWriter
Write a single cluster model to a file.
#####
DBaseExampleSource
This operator can read dbase files. It uses Stefan Haustein's kdb tools.
#####
DatabaseExampleSetWriter
<p>This operator writes an <i>ExampleSet</i> into an SQL
 database. The user can specify the database connection and a table name. Please note
 that the table will be created during writing if it does not exist.</p>
 
 <p>The most convenient way of defining the necessary parameters is the 
 configuration wizard. The most important parameters (database URL and user name) will
 be automatically determined by this wizard. At the end, you only have to define
 the table name and then you are ready.</p>

 <p>
 This operator only supports the writing of the complete example set consisting of
 all regular and special attributes and all examples. If this is not desired perform some
 preprocessing operators like attribute or example filter before applying this operator.
 </p>
#####
DatabaseExampleSource
<p>This operator reads an <i>ExampleSet</i> from an SQL
 database. The SQL query can be passed to RapidMiner via a parameter or, in case of
 long SQL statements, in a separate file. Please note that column names are
 often case sensitive. Databases may behave differently here.</p>
 
 <p>The most convenient way of defining the necessary parameters is the 
 configuration wizard. The most important parameters (database URL and user name) will
 be automatically determined by this wizard and it is also possible to define
 the special attributes like labels or ids.</p>
 
 <p>Please note that this operator supports two basic working modes:</p>
 <ol>
 <li>reading the data from the database and creating an example table in main memory</li>
 <li>keeping the data in the database and directly working on the database table </li>
 </ol>
 <p>The latter possibility will be turned on by the parameter &quot;work_on_database&quot;.
 Please note that this working mode is still regarded as experimental and errors might
 occur. In order to ensure proper data changes the database working mode is only allowed
 on a single table which must be defined with the parameter &quot;table_name&quot;. 
 IMPORTANT: If you encounter problems during data updates (e.g. messages that the result set is not 
 updatable) you probably have to define a primary key for your table.</p>
 
 <p>If you are not directly working on the database, the data will be read with an arbitrary
 SQL query statement (SELECT ... FROM ... WHERE ...) defined by &quot;query&quot; or &quot;query_file&quot;.
 The memory mode is the recommended way of using this operator. This is especially important for
 following operators like learning schemes which would often load (most of) the data into main memory
 during the learning process. In these cases a direct working on the database is not recommended
 anyway.</p>
 
 <h5>Warning</h5>
 As the java <code>ResultSetMetaData</code> interface does not provide
 information about the possible values of nominal attributes, the internal
 indices the nominal values are mapped to will depend on the ordering
 they appear in the table. This may cause problems only when processes are
 split up into a training process and an application or testing process.
 For learning schemes which are capable of handling nominal attributes, this
 is not a problem. If a learning scheme like a SVM is used with nominal data,
 RapidMiner pretends that nominal attributes are numerical and uses indices for the
 nominal values as their numerical value. A SVM may perform well if there are
 only two possible values. If a test set is read in another process, the
 nominal values may be assigned different indices, and hence the SVM trained
 is useless. This is not a problem for label attributes, since the classes can
 be specified using the <code>classes</code> parameter and hence, all
 learning schemes intended to use with nominal data are safe to use.
#####
DirectMailingExampleSetGenerator
Generates a random example set for testing purposes. The data represents a direct mailing
 example set.
#####
ExampleSetGenerator
Generates a random example set for testing purposes. Uses a subclass of
 <i>TargetFunction</i> to create the examples from the attribute values.
 Possible target functions are: random, sum (of all attributes), polynomial
 (of the first three attributes, degree 3), non linear, sinus, sinus frequency
 (like sinus, but with frequencies in the argument), random classification,
 sum classification (like sum, but positive for positive sum and negative for
 negative sum), interaction classification (positive for negative x or
 positive y and negative z), sinus classification (positive for positive sinus
 values).
#####
ExampleSetWriter
Writes values of all examples in an <i>ExampleSet</i> to a file. Dense,
 sparse, and user defined formats (specified by the parameter 'format') can be
 used. Attribute description files may be generated for dense and sparse
 format as well. These formats can be read using the <i>ExampleSource</i> and
 <i>SparseFormatExampleSource</i> operators.
 
 <dl>
 <dt>dense:</dt>
 <dd> Each line of the generated data file is of the form<br/> <center>
 
 <pre>
 regular attributes &lt;special attributes&gt;
 </pre>
 
 </center> For example, each line could have the form <center>
 
 <pre>
 value1 value2 ... valueN &lt;id&gt; &lt;label&gt; &lt;prediction&gt; ... &lt;confidences&gt;
 </pre>
 
 </center> Values in parenthesis are optional and are only printed if they are
 available. The confidences are only given for nominal predictions. Other
 special attributes might be the example weight or the cluster number. </dd>
 <dt>sparse:</dt>
 <dd>Only non 0 values are written to the file, prefixed by a column index.
 See the description of <i>SparseFormatExampleSource</i> for details. </dd>
 <dt>special:</dt>
 <dd>Using the parameter 'special_format', the user can specify the exact
 format. The $ sign has a special meaning and introduces a command (the
 following character) Additional arguments to this command may be supplied
 enclosing it in square brackets.
 <dl>
 <dt>$a:</dt>
 <dd> All attributes separated by the default separator</dd>
 <dt>$a[separator]:</dt>
 <dd> All attributes separated by separator</dd>
 <dt>$s[separator][indexSeparator]:</dt>
 <dd> Sparse format. For all non zero attributes the following strings are
 concatenated: the column index, the value of indexSeparator, the attribute
 value. Attributes are separated by separator.</dd>
 <dt>$v[name]:</dt>
 <dd> The value of the attribute with the given name (both regular and special
 attributes)</dd>
 <dt>$k[index]:</dt>
 <dd> The value of the attribute with the given index</dd>
 <dt>$l:</dt>
 <dd> The label</dd>
 <dt>$p:</dt>
 <dd> The predicted label</dd>
 <dt>$d:</dt>
 <dd> All prediction confidences for all classes in the form conf(class)=value</dd>
 <dt>$d[class]:</dt>
 <dd> The prediction confidence for the defined class as a simple number</dd>
 <dt>$i:</dt>
 <dd> The id</dd>
 <dt>$w:</dt>
 <dd> The weight</dd>
 <dt>$b:</dt>
 <dd> The batch number</dd>
 <dt>$n:</dt>
 <dd> The newline character</dd>
 <dt>$t:</dt>
 <dd> The tabulator character</dd>
 <dt>$$:</dt>
 <dd> The dollar sign</dd>
 <dt>$[:</dt>
 <dd> The '[' character</dd>
 <dt>$]:</dt>
 <dd> The ']' character</dd>
 </dl>
 Make sure the format string ends with $n if you want examples to be separated
 by newlines!</dd>
 </dl>
#####
ExampleSource
<p>
 This operator reads an example set from (a) file(s). Probably you can use the
 default parameter values for the most file formats (including the format
 produced by the ExampleSetWriter, CSV, ...). Please refer to section
 <i>First steps/File formats</i> for details on the
 attribute description file set by the parameter <var>attributes</var> used
 to specify attribute types.
 </p>
 
 <p>
 This operator supports the reading of data from multiple source files. Each
 attribute (including special attributes like labels, weights, ...) might be
 read from another file. Please note that only the minimum number of lines of
 all files will be read, i.e. if one of the data source files has less lines
 than the others, only this number of examples will be read.
 </p>
 
 <p>
 The split points can be defined with regular expressions (please refer to the
 Java API). The default split parameter &quot;,\s*|;\s*|\s+&quot; should work
 for most file formats. This regular expression describes the following column
 separators
 <ul>
 <li>the character &quot;,&quot; followed by a whitespace of arbitrary length (also no white space)</li>
 <li>the character &quot;;&quot; followed by a whitespace of arbitrary length (also no white space)</li>
 <li>a whitespace of arbitrary length (min. 1)</li> 
 </ul>
 A logical XOR is defined by &quot;|&quot;. Other useful separators might be
 &quot;\t&quot; for tabulars, &quot; &quot; for a single whitespace, and
 &quot;\s&quot; for any whitespace.
 </p>
 
 <p>
 Quoting is also possible with &quot;. However, since using quotes might slow down
 the parsing it is therefore recommended to ensure that the
 split characters are not included in the data columns and that quotes are not
 needed.
 </p>
 
 <p>
 Additionally you can specify comment characters which can be used at
 arbitrary locations of the data lines. Any content after the comment character
 will be ignored. Unknown attribute values can be marked with empty strings 
 (if this is possible for your column separators) or by a question mark (recommended).
 </p>
#####
ExcelExampleSetWriter
<p>This operator can be used to write data into Microsoft Excel spreadsheets. 
 This operator creates Excel files readable by Excel 95, 97, 2000, XP, 2003 
 and newer. Missing data values are indicated by empty cells.</p>
#####
ExcelExampleSource
<p>This operator can be used to load data from Microsoft Excel spreadsheets. 
 This operator is able to reads data from Excel 95, 97, 2000, XP, and 2003.
 The user has to define which of the spreadsheets in the workbook should be
 used as data table. The table must have a format so that each line is an example
 and each column represents an attribute. Please note that the first line might
 be used for attribute names which can be indicated by a parameter.</p>
 
 <p>The data table can be placed anywhere on the sheet and is allowed to
 contain arbitrary formatting instructions, empty rows, and empty columns. Missing data values
 are indicated by empty cells or by cells containing only &quot;?&quot;.</p>
#####
GnuplotWriter
Writes the data generated by a ProcessLogOperator to a file in gnuplot
 format.
#####
IOContainerReader
Reads all elements of an <i>IOContainer</i> from a file. The file must be written
 by an <i>IOContainerWriter</i>.
 
 The operator additionally supports to read text from a logfile, which will be
 given to the RapidMiner <i>LogService</i>. Hence, if you add a IOContainerWriter
 to the end of an process and set the logfile in the process root
 operator, the output of applying the IOContainerReader will be quite similar
 to what the original process displayed.
#####
IOContainerWriter
Writes all elements of the current <i>IOContainer</i>, i.e. all objects
 passed to this operator, to a file. Although this operator uses an XML serialization 
 mechanism, the files produced for different RapidMiner versions might not be compatible. At least 
 different Java versions should not be a problem anymore.
#####
IOObjectReader
Generic reader for all types of IOObjects. Reads an IOObject from a file.
#####
IOObjectWriter
Generic writer for all types of IOObjects. Writes one of the input objects into a given file.
#####
MassiveDataGenerator
Generates huge amounts of data in either sparse or dense format. This
 operator can be used to check if huge amounts of data can be handled by RapidMiner
 for a given process setup without creating the correct format / writing
 special purpose input operators.
#####
ModelLoader
Reads a <i>Model</i> from a file that was generated
 by an operator like <i>Learner</i> in a
 previous process. Once a model is generated, it can be applied several
 times to newly acquired data using a model loader, an <i>ExampleSource</i>,
 and a <i>ModelApplier</i>.
#####
ModelWriter
<p>Writes the input model in the file specified by the corresponding parameter.
 Since models are often written into files and loaded and applied in other 
 processes or applications, this operator offers three different writing
 modes for models:</p>
 
 <ul>
 <li><em>XML</em>: in this mode, the models are written as plain text XML files. The file size
 is usually the biggest in this mode (might be several hundred mega bytes so you should
 be cautious) but this model type has the advantage that the user can inspect and change
 the files.</li>
 <li><em>XML Zipped (default)</em>: In this mode, the models are written as zipped XML files. Users 
 can simply unzip the files and read or change the contents. The file sizes are smallest for
 most models. For these reasons, this mode is the default writing mode for models although
 the loading times are the longest due to the XML parsing and unzipping.</li>
 <li><em>Binary</em>: In this mode, the models are written in an proprietary binary format. 
 The resulting model files cannot be inspected by the user and the file sizes are usually 
 slightly bigger then for the zipped XML files. The loading time, however, is smallers than 
 the time needed for the other modes.</li>
 </ul>
 
 <p>This operator is also able to keep old files if the overwriting flag is set to false.
 However, this could also be achieved by using some of the parameter macros provided by
 RapidMiner like %{t} or %{a} (please refer to the tutorial section about macros).</p>
#####
MultipleLabelGenerator
Generates a random example set for testing purposes with more than one label.
#####
NominalExampleSetGenerator
Generates a random example set for testing purposes. All attributes have only
 (random) nominal values and a classification label.
#####
ParameterSetLoader
Reads a set of parameters from a file that was written by a
 <i>ParameterOptimizationOperator</i>. It can
 then be applied to the operators of the process using a
 <i>ParameterSetter</i>.
#####
ParameterSetWriter
Writes a parameter set into a file. This can be created by one of the
 parameter optimization operators, e.g.
 <i>GridSearchParameterOptimizationOperator</i>.
 It can then be applied to the operators of the process using a
 <i>ParameterSetter</i>.
#####
PerformanceLoader
Reads a performance vector from a given file. This performance vector must
 have been written before with a <i>PerformanceWriter</i>.
#####
PerformanceWriter
Writes the input performance vector in a given file. You also might want to
 use the <i>ResultWriter</i> operator which
 writes all current results in the main result file.
#####
ResultWriter
This operator can be used at each point in an operator chain. It returns all
 input it receives without any modification. Every input object which
 implements the <i>ResultObject</i> interface (which
 is the case for almost all objects generated by the core RapidMiner operators) will
 write its results to the file specified by the parameter <var>result_file</var>.
 If the definition of this parameter is ommited then the global result file parameter
 with the same name of the ProcessRootOperator 
 (the root of the process) will be used. If this file is also not specified
 the results are simply written to the console (standard out).
#####
SPSSExampleSource
This operator can read spss files.
#####
SimpleExampleSource
<p>
 This operator reads an example set from (a) file(s). Probably you can use the
 default parameter values for the most file formats (including the format
 produced by the ExampleSetWriter, CSV, ...). In fact, in many cases this operator
 is more appropriate for CSV based file formats than the <i>CSVExampleSource</i> operator
 itself.
 </p>
 
 <p>
 In contrast to the usual ExampleSource operator this operator is able to read the
 attribute names from the first line of the data file. However, there is one
 restriction: the data can only be read from one file instead of multiple
 files. If you need a fully flexible operator for data loading you should use 
 the more powerful ExampleSource operator.
 </p>
 
 <p>
 The column split points can be defined with regular expressions (please refer to the
 Java API). The default split parameter &quot;,\s*|;\s*|\s+&quot; should work
 for most file formats. This regular expression describes the following column
 separators
 <ul>
 <li>the character &quot;,&quot; followed by a whitespace of arbitrary length (also no white space)</li>
 <li>the character &quot;;&quot; followed by a whitespace of arbitrary length (also no white space)</li>
 <li>a whitespace of arbitrary length (min. 1)</li>
 </ul>
 A logical XOR is defined by &quot;|&quot;. Other useful separators might be
 &quot;\t&quot; for tabulars, &quot; &quot; for a single whitespace, and
 &quot;\s&quot; for any whitespace.
 </p>
 
 <p>
 Quoting is also possible with &quot;. However, using quotes slows down
 parsing and is therefore not recommended. The user should ensure that the
 split characters are not included in the data columns and that quotes are not
 needed. Additionally you can specify comment characters which can be used at
 arbitrary locations of the data lines. Unknown attribute values can be marked
 with empty strings or a question mark.
 </p>
#####
SparseFormatExampleSource
Reads an example file in sparse format, i.e. lines have the form<br/>
 <center>
 
 <pre>
 label index:value index:value index:value...
 </pre>
 
 </center><br/> Index may be an integer (starting with 1) for the regular
 attributes or one of the prefixes specified by the parameter list
 <code>prefix_map</code>. Four possible <code>format</code>s are
 supported
 <dl>
 <dt>format_xy:</dt>
 <dd>The label is the last token in each line</dd>
 <dt>format_yx:</dt>
 <dd>The label is the first token in each line</dd>
 <dt>format_prefix:</dt>
 <dd>The label is prefixed by 'l:'</dd>
 <dt>format_separate_file:</dt>
 <dd>The label is read from a separate file specified by
 <code>label_file</code></dd>
 <dt>no_label:</dt>
 <dd>The example set is unlabeled.</dd>
 </dl>
 A detailed introduction to the sparse file format is given in section
 <i>First steps/File formats/Data files</i>.
#####
TeamProfitExampleSetGenerator
Generates a random example set for testing purposes. The data represents a team profit
 example set.
#####
ThresholdLoader
Reads a threshold from a file. The first line must hold the threshold, the
 second the value of the first class, and the second the value of the second
 class. This file can be written in another process using the
 <i>ThresholdWriter</i>.
#####
ThresholdWriter
Writes the given threshold into a file. The first line holds the threshold,
 the second the value of the first class, and the second the value of the
 second class. This file can be read in another process using the
 <i>ThresholdLoader</i>.
#####
TransfersExampleSetGenerator
Generates a random example set for testing purposes. The data represents a team profit
 example set.
#####
UpSellingExampleSetGenerator
Generates a random example set for testing purposes. The data represents an up-selling
 example set.
#####
WekaModelLoader
This operator reads in model files which were saved from the Weka toolkit. For models
 learned within RapidMiner please use always the <i>ModelLoader</i> operator even it the 
 used learner was originally a Weka learner.
#####
XrffExampleSetWriter
<p>Writes values of all examples into an XRFF file which can be used
 by the machine learning library Weka. The XRFF format is described in the 
 <i>XrffExampleSource</i> operator which is able to read XRFF files to
 make them usable with RapidMiner.</p>
 
 <p>Please note that writing attribute weights is not supported, please use
 the other RapidMiner operators for attribute weight loading and writing for this
 purpose.</p>
#####
XrffExampleSource
<p>This operator can read XRFF files known from Weka.
 The XRFF (eXtensible attribute-Relation File Format) is an XML-based extension of the ARFF format
 in some sense similar to the original RapidMiner file format for attribute description files (.aml).</p>
 
 <p>Here you get a small example for the IRIS dataset represented as XRFF file:</p>

 <pre>
 &lt;?xml version="1.0" encoding="utf-8"?&gt;
 &lt;dataset name="iris" version="3.5.3"&gt;
  &lt;header&gt;
     &lt;attributes&gt;
        &lt;attribute name="sepallength" type="numeric"/&gt;
        &lt;attribute name="sepalwidth" type="numeric"/&gt;
        &lt;attribute name="petallength" type="numeric"/&gt;
        &lt;attribute name="petalwidth" type="numeric"/&gt;
        &lt;attribute class="yes" name="class" type="nominal"&gt;
           &lt;labels&gt;
              &lt;label&gt;Iris-setosa&lt;/label&gt;
              &lt;label&gt;Iris-versicolor&lt;/label&gt;
              &lt;label&gt;Iris-virginica&lt;/label&gt;
           &lt;/labels&gt;
        &lt;/attribute&gt;
     &lt;/attributes&gt;
  &lt;/header&gt;

  &lt;body&gt;
     &lt;instances&gt;
        &lt;instance&gt;
           &lt;value&gt;5.1&lt;/value&gt;
           &lt;value&gt;3.5&lt;/value&gt;
           &lt;value&gt;1.4&lt;/value&gt;
           &lt;value&gt;0.2&lt;/value&gt;
           &lt;value&gt;Iris-setosa&lt;/value&gt;
        &lt;/instance&gt;
        &lt;instance&gt;
           &lt;value&gt;4.9&lt;/value&gt;
           &lt;value&gt;3&lt;/value&gt;
           &lt;value&gt;1.4&lt;/value&gt;
           &lt;value&gt;0.2&lt;/value&gt;
           &lt;value&gt;Iris-setosa&lt;/value&gt;
        &lt;/instance&gt;
        ...
     &lt;/instances&gt;
  &lt;/body&gt;
 &lt;/dataset&gt;
 </pre>
 
 <p>Please note that the sparse XRFF format is currently not supported, please use one of the 
 other options for sparse data files provided by RapidMiner.</p>

 <p>Since the XML representation takes up considerably more space since the data is wrapped
 into XML tags, one can also compress the data via gzip. RapidMiner automatically recognizes a file 
 being gzip compressed, if the file's extension is .xrff.gz instead of .xrff.</p>

 <p>Similar to the native RapidMiner data definition via .aml and almost arbitrary data files, the XRFF 
 format contains some additional features. Via the class="yes" attribute in the attribute 
 specification in the header, one can define which attribute should used as a prediction label 
 attribute. Although the RapidMiner terminus for such classes is &quot;label&quot; instead of 
 &quot;class&quot; we support the terminus class in order to not break compatibility with
 original XRFF files.</p>
  
 <p>Please note that loading attribute weights is currently not supported, please use
 the other RapidMiner operators for attribute weight loading and writing for this
 purpose.</p>

 <p>Instance weights can be defined via a weight XML attribute in each instance tag. 
 By default, the weight is 1. Here's an example:</p>

 <pre>
 &lt;instance weight="0.75"&gt;
  &lt;value&gt;5.1&lt;/value&gt;
  &lt;value&gt;3.5&lt;/value&gt;
  &lt;value&gt;1.4&lt;/value&gt;
  &lt;value&gt;0.2&lt;/value&gt;
  &lt;value&gt;Iris-setosa&lt;/value&gt;
 &lt;/instance&gt;
 </pre>
 
 <p>Since the XRFF format does not support id attributes one have to use one of the RapidMiner
 operators in order to change on of the columns to the id column if desired. This has to be done
 after loading the data.</p>
#####

AdaBoost
This AdaBoost implementation can be used with all learners available in RapidMiner, not only
 the ones which originally are part of the Weka package.
#####
AdditiveRegression
<p>This operator uses  regression learner as a base learner. The learner starts with a default
 model (mean or mode) as a first prediction model. In each iteration it learns a 
 new base model and applies it to the example set. Then, the residuals of the labels are
 calculated and the next base model is learned. The learned meta model predicts the label
 by adding all base model predictions.</p>
#####
AgglomerativeClustering
This operator performs generic agglomorative clustering based on a set of ids and a similarity measure. The algorithm implemented here is currently
 very simple and not very efficient (cubic).
#####
AgglomerativeFlatClustering
This operator performs generic agglomorative clustering based on a set of ids and a similarity measure. Clusters are merged as long as their number
 is lower than a given maximum number of clusters. The algorithm implemented here is currently very simple and not very efficient (cubic).
#####
AssociationRuleGenerator
<p>This operator generates association rules from frequent item sets. 
 In RapidMiner, the process of frequent item set mining is divided
 into two parts: first, the generation of frequent item sets and
 second, the generation of association rules from these sets.</p>
 
 <p>For the generation of frequent item sets, you can use for example
 the operator <i>FPGrowth</i>. The result will be a set of frequent item
 sets which could be used as input for this operator.</p>
#####
AttributeBasedVote
AttributeBasedVotingLearner is very lazy. Actually it does not learn at all but creates an
 <i>AttributeBasedVotingModel</i>. This model simply calculates the average of the
 attributes as prediction (for regression) or the mode of all attribute values 
 (for classification). AttributeBasedVotingLearner is especially useful if it is used
 on an example set created by a meta learning scheme, e.g. by <i>Vote</i>.
#####
Bagging
This Bagging implementation can be used with all learners available in RapidMiner, not only
 the ones which originally are part of the Weka package.
#####
BasicRuleLearner
This operator builds an unpruned rule set of classification rules. It is based on 
 the paper Cendrowska, 1987: PRISM: An algorithm for inducing modular rules.
#####
BayesianBoosting
<p>This operator trains an ensemble of classifiers for boolean target
 attributes. In each iteration the training set is reweighted, so that
 previously discovered patterns and other kinds of prior knowledge are
 &quot;sampled out&quot; {@rapidminer.cite Scholz/2005b}. An inner classifier,
 typically a rule or decision tree induction algorithm, is sequentially
 applied several times, and the models are combined to a single global model.
 The number of models to be trained maximally are specified by the parameter
 <code>iterations</code>.</p>
 
 <p>If the parameter <code>rescale_label_priors</code> is set, then the example
 set is reweighted, so that all classes are equally probable (or frequent).
 For two-class problems this turns the problem of fitting models to maximize
 weighted relative accuracy into the more common task of classifier induction
 {@rapidminer.cite Scholz/2005a}. Applying a rule induction algorithm as an inner
 learner allows to do subgroup discovery. This option is also recommended for
 data sets with class skew, if a &quot;very weak learner&quot; like a decision
 stump is used. If <code>rescale_label_priors</code> is not set, then the
 operator performs boosting based on probability estimates.</p>
 
 <p>The estimates used by this operator may either be computed using the same set
 as for training, or in each iteration the training set may be split randomly,
 so that a model is fitted based on the first subset, and the probabilities
 are estimated based on the second. The first solution may be advantageous in
 situations where data is rare. Set the parameter
 <code>ratio_internal_bootstrap</code> to 1 to use the same set for training
 as for estimation. Set this parameter to a value of lower than 1 to use the
 specified subset of data for training, and the remaining examples for
 probability estimation.</p>
 
 <p>If the parameter <code>allow_marginal_skews</code> is <em>not</em> set,
 then the support of each subset defined in terms of common base model
 predictions does not change from one iteration to the next. Analogously the
 class priors do not change. This is the procedure originally described in
 {@rapidminer.cite Scholz/2005b} in the context of subgroup discovery.</p>
 
 <p>Setting the <code>allow_marginal_skews</code> option to <code>true</code>
 leads to a procedure that changes the marginal weights/probabilities of
 subsets, if this is beneficial in a boosting context, and stratifies the two
 classes to be equally likely. As for AdaBoost, the total weight upper-bounds
 the training error in this case. This bound is reduced more quickly by the
 BayesianBoosting operator, however.</p>
 
 <p>In sum, to reproduce the sequential sampling, or knowledge-based sampling, 
 from {@rapidminer.cite Scholz/2005b} for subgroup discovery, two of the 
 default parameter settings of this operator have to be changed: 
 <code>rescale_label_priors</code> must 
 be set to <code>true</code>, and <code>allow_marginal_skews</code> must 
 be set to <code>false</code>. In addition, a boolean (binomial) label 
 has to be used.</p>
  
 <p>The operator requires an example set as its input. To sample out prior
 knowledge of a different form it is possible to provide another model as an
 optional additional input. The predictions of this model are used to weight
 produce an initial weighting of the training set. The ouput of the operator
 is a classification model applicable for estimating conditional class
 probabilities or for plain crisp classification. It contains up to the
 specified number of inner base models. In the case of an optional initial
 model, this model will also be stored in the output model, in order to
 produce the same initial weighting during model application.</p>
#####
BestRuleInduction
This operator returns the best rule regarding WRAcc using exhaustive search.
 Features like the incorporation of other metrics and the search for more than
 a single rule are prepared.
 
 The search strategy is BFS, with save pruning whenever applicable. This
 operator can easily be extended to support other search strategies.
#####
Binary2MultiClassLearner
A metaclassifier for handling multi-class datasets with 2-class classifiers. This class
 supports several strategies for multiclass classification including procedures which are 
 capable of using error-correcting output codes for increased accuracy.
#####
BregmanHardClustering
This operator represents an implementation of the Bregman Hard Clustering.
#####
CHAID
The CHAID decision tree learner works like the 
 <i>DecisionTreeLearner</i>
 with one exception: it used a chi squared based criterion
 instead of the information gain or gain ratio criteria.
#####
ClassificationByRegression
For a classified dataset (with possibly more than two classes) builds a
 classifier using a regression method which is specified by the inner
 operator. For each class <i>i</i> a regression model is trained after
 setting the label to <i>+1</i> if the label equals <i>i</i> and
 to <i>-1</i> if it is not. Then the regression models are combined into
 a classification model. In order to determine the prediction for an unlabeled
 example, all models are applied and the class belonging to the regression
 model which predicts the greatest value is chosen.
#####
ClusterModel2ExampleSet
Labels an example set with the cluster ids from a given cluster model.
#####
ClusterModel2Similarity
This operator converts a (hierarchical) cluster model to a similarity measure.
#####
CostBasedThresholdLearner
<p>This operator uses a set of class weights and also allows a weight for the fact
 that an example is not classified at all (marked as unknown). Based on the
 predictions of the model of the inner learner this operator optimized
 a set of thresholds regarding the defined weights.</p>
 
 <p>
 This operator might be very useful in cases where it is better to not classify
 an example then to classify it in a wrong way. This way, it is often possible
 to get very high accuracies for the remaining examples (which are actually
 classified) for the cost of having some examples which must still be manually
 classified. 
 </p>
#####
DBScanClustering
This operator represents a simple implementation of the DBSCAN algorithm. {@rapidminer.cite Ester/etal/96a}).
#####
DecisionStump
This operator learns decision stumps, i.e. a small decision tree with only
 one single split. This decision stump works on both numerical and nominal
 attributes.
#####
DecisionTree
<p>This operator learns decision trees from both nominal and numerical data.
 Decision trees are powerful classification methods which often can also
 easily be understood. This decision tree learner works similar to Quinlan's
 C4.5 or CART.</p>
 
 <p>The actual type of the tree is determined by the criterion, e.g. using 
 gain_ratio or Gini for CART / C4.5.</p>
#####
DefaultLearner
This learner creates a model, that will simply predict a default value for
 all examples, i.e. the average or median of the true labels (or the mode in
 case of classification) or a fixed specified value. This learner can be used
 to compare the results of &quot;real&quot; learning schemes with guessing.
#####
EvoSVM
<p>This is a SVM implementation using an evolutionary algorithm (ES) to solve
 the dual optimization problem of a SVM. It turns out that on many datasets
 this simple implementation is as fast and accurate as the usual SVM
 implementations. In addition, it is also capable of learning with Kernels 
 which are not positive semi-definite and can also be used for multi-objective
 learning which makes the selection of C unecessary before learning.</p> 
 
 <p>Mierswa, Ingo. Evolutionary Learning with Kernels: A Generic Solution for Large 
 Margin Problems. In Proc. of the Genetic and Evolutionary Computation Conference 
 (GECCO 2006), 2006.</p>
#####
ExampleSet2ClusterConstraintList
Creates a ClusterConstraintList of the specified type from a (possibly partially) labeled ExampleSet. For the type
 'link' you can choose, if you want the LinkClusterConstraints to be created randomly or orderly, always bounded by
 the maximal number of constraints to create. Choosing 'random walk' the Must-Link-constraints for each label will
 form a connected component.
#####
ExampleSet2ClusterModel
Operator that clusters items along one given nominal attribute.
#####
ExampleSet2Similarity
This class represents an operator that creates a similarity measure based on an ExampleSet.
#####
FPGrowth
<p>This operator calculates all frequent items sets from a data set by building 
 a FPTree data structure on the transaction data base. This is a very compressed
 copy of the data which in many cases fits into main memory even for large
 data bases. From this FPTree all frequent item set are derived. A major advantage
 of FPGrowth compared to Apriori is that it uses only 2 data scans and is therefore
 often applicable even on large data sets.</p>
 
  <p>Please note that the given data set is only allowed to contain binominal attributes,
  i.e. nominal attributes with only two different values. Simply use the provided
  preprocessing operators in order to transform your data set. The necessary operators
  are the discretization operators for changing the value types of numerical
  attributes to nominal and the operator Nominal2Binominal for transforming nominal
  attributes into binominal / binary ones.
  The frequent item sets are mined for the positive entries in your data base,
  i.e. for those nominal values which are defined as positive in your data base.
  If you use an attribute description file (.aml) for the <i>ExampleSource</i> operator
  this corresponds to the second value which is defined via the classes attribute or inner
  value tags.</p>
#####
FlattenClusterModel
Creates a flat cluster model from a hierarchical one by expanding nodes in the order of their weight until the desired number of clusters is
 reached.
#####
GPLearner
Gaussian Process (GP) Learner. The GP is a probabilistic method 
 both for classification and regression.
#####
HyperHyper
This is a minimal SVM implementation. The model is built with only one positive
 and one negative example. Typically this operater is used in combination with a
 boosting method.
#####
ID3
This operator learns decision trees without pruning using nominal
 attributes only.
 Decision trees are powerful classification methods which often can also
 easily be understood. This decision tree learner works similar to Quinlan's
 ID3.
#####
ID3Numerical
This operator learns decision trees without pruning using both nominal
 and numerical attributes.
 Decision trees are powerful classification methods which often can also
 easily be understood. This decision tree learner works similar to Quinlan's
 ID3.
#####
IteratingGSS
This operator implements the IteratingGSS algorithmus presented in the diploma thesis 'Effiziente Entdeckung unabhaengiger Subgruppen in grossen Datenbanken' at the Department of Computer Science,
 University of Dortmund.
#####
JMySVMLearner
This learner uses the Java implementation of the support vector machine
 <em>mySVM</em> by Stefan R&uuml;ping. This learning method can be used for
 both regression and classification and provides a fast algorithm and good
 results for many learning tasks.
#####
KMeans
This operator represents a simple implementation of k-means.
#####
KMedoids
Simple implementation of k-medoids.
#####
KernelKMeans
Simple implementation of kernel k-means {@rapidminer.cite Dhillon/etal/2004a}.
#####
KernelLogisticRegression
This operator determines a logistic regression model.
#####
LibSVMLearner
Applies the <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">libsvm</a>
 learner by Chih-Chung Chang and Chih-Jen Lin. The SVM is a powerful method
 for both classification and regression. This operator supports the SVM types
 <code>C-SVC</code> and <code>nu-SVC</code> for classification tasks and
 <code>epsilon-SVR</code> and <code>nu-SVR</code> for regression tasks.
 Supports also multiclass learning and probability estimation based on Platt
 scaling for proper confidence values after applying the learned model on a
 classification data set.
#####
LinearRegression
<p>This operator calculates a linear regression model. It uses the Akaike criterion 
  for model selection.</p>
#####
LogisticRegression
This operator determines a logistic regression model.
#####
MPCKMeans
This is an implementation of the "Metric Pairwise Constraints K-Means" algorithm (see "Mikhail Bilenko, Sugato Basu,
 and Raymond J. Mooney. Integrating constraints and metric learning in semi-supervised clustering. In Proceedings of
 the 21st International Conference on Machine Learning, ICML, pages 8188, Banff, Canada, July 2004.") that uses a
 list of LinkClusterConstraints created from a (possibly partially) labeled ExampleSet to learn a parameterized
 euklidean distance metric.
#####
MetaCost
This operator uses a given cost matrix to compute label predictions
 according to classification costs. The method used by this operator
 is similar to MetaCost as described by Pedro Domingos.
#####
MultiCriterionDecisionStump
A DecisionStump clone that allows to specify different utility functions.
 It is quick for nominal attributes, but does not yet apply pruning for continuos attributes.  
 Currently it can only handle boolean class labels.
#####
MyKLRLearner
This is the Java implementation of <em>myKLR</em> by Stefan R&uuml;ping.
 myKLR is a tool for large scale kernel logistic regression based on the
 algorithm of Keerthi/etal/2003 and the code of mySVM.
#####
NaiveBayes
NaiveBayes is a learner based on the Bayes theorem. If the attributes
 are fully independent, it is the theoretically best learner which 
 could be used. Although this assumption is often not fulfilled it
 delivers quite predictions. This operator uses normal distributions 
 in order to estimate real-valued distributions of data.
 Numerical missing values will be ignored, nominal missing values will
 be treated as nominal category
#####
NearestNeighbors
A simple k nearest neighbor implementation.
#####
NeuralNet
<p>This operator learns a model by means of a feed-forward neural network. The learning is
 done via backpropagation. The user can define the structure of the neural network with the
 parameter list &quot;hidden_layer_types&quot;. Each list entry describes a new hidden
 layer. The key of each entry must correspond to the layer type which must be one out of</p>
 
 <ul>
 <li>linear</li>
 <li>sigmoid (default)</li>
 <li>tanh</li>
 <li>sine</li>
 <li>logarithmic</li>
 <li>gaussian</li>
 </ul>
 
 <p>The key of each entry must be a number defining the size of the hidden layer. A size value
 of -1 or 0 indicates that the layer size should be calculated from the number of attributes
 of the input example set. In this case, the layer size will be set to 
 (number of attributes + number of classes) / 2 + 1.</p>
 
 <p>If the user does not specify any hidden layers, a default hidden layer with 
 sigmoid type and size (number of attributes + number of classes) / 2 + 1 will be created and 
 added to the net.</p>
 
 <p>The type of the input nodes is sigmoid. The type of the output node is sigmoid is the 
 learning data describes a classification task and linear for numerical regression tasks.</p>
#####
OneR
This operator concentrates on one single attribute and determines the best splitting terms
 for minimizing the training error. The result will be a single rule containing all these
 terms.
#####
Perceptron
The perceptron is a type of artificial neural network invented in 1957 by Frank Rosenblatt. 
 It can be seen as the simplest kind of feedforward neural network: a linear classifier.
 Beside all biological analogies, the single layer perceptron is simply a linear classifier
 which is efficiently trained by a simple update rule: for all wrongly classified data points,
 the weight vector is either increased or decreased by the corresponding example values.
#####
PsoSVM
This is a SVM implementation using a particle swarm optimization (PSO)
 approach to solve the dual optimization problem of a SVM. It turns out that
 on many datasets this simple implementation is as fast and accurate as the
 usual SVM implementations.
#####
RVMLearner
Relevance Vector Machine (RVM) Learner. The RVM is a probabilistic method 
  both for classification and regression. The implementation of the relevance 
  vector machine is based on the original algorithm described by Tipping/2001.
  The fast version of the marginal likelihood maximization (Tipping/Faul/2003) 
  is also available if the parameter &quot;rvm_type&quot; is set to 
  &quot;Constructive-Regression-RVM&quot;.
#####
RandomFlatClustering
Returns a random clustering. Note that this algorithm does not garantuee that all clusters are non-empty.
#####
RandomForest
This operators learns a random forest. The resulting forest model contains serveral 
 single random tree models.
#####
RandomTree
<p>
 This operator learns decision trees from both nominal and numerical data.
 Decision trees are powerful classification methods which often can also
 easily be understood. The random tree learner works similar to Quinlan's
 C4.5 or CART but it selects a random subset of attributes before it is 
 applied. The size of the subset is defined by the parameter subset_ratio.
 </p>
#####
RelevanceTree
Learns a pruned decision tree based on arbitrary feature relevance measurements
 defined by an inner operator (use for example <i>InfoGainRatioWeighting</i>
 for C4.5 and <i>ChiSquaredWeighting</i> for CHAID. Works only for nominal
 attributes.
#####
RuleLearner
<p>This operator works similar to the propositional rule learner named 
 Repeated Incremental Pruning to Produce Error Reduction (RIPPER, Cohen 1995). 
 Starting with the less prevalent classes, the algorithm iteratively grows
 and prunes rules until there are no positive examples left or the error 
 rate is greater than 50%.</p>
 
 <p>In the growing phase, for each rule greedily conditions are added to the rule 
 until the rule is perfect (i.e. 100% accurate). The procedure tries every 
 possible value of each attribute and selects the condition with highest 
 information gain.</p>
 
 <p>In the prune phase, for each rule any final sequences of the antecedents is 
 pruned with the pruning metric p/(p+n).</p>
#####
SimilarityComparator
Operator that compares two similarity measures using diverse metrics.
#####
Stacking
This class uses n+1 inner learners and generates n different models
 by using the last n learners. The predictions of these n models are
 taken to create n new features for the example set, which is finally
 used to serve as an input of the first inner learner.
#####
SubgroupDiscovery
Subgroup discovery learner.
#####
SupportVectorClustering
An implementation of Support Vector Clustering based on {@rapidminer.cite BenHur/etal/2001a}.
#####
TopDownClustering
A top-down generic clustering that can be used with any (flat) clustering as
 inner operator. Note though, that the outer operator cannot set or get the
 maximal number of clusters, the inner operator produces. These value has to
 be set in the inner operator.
#####
TopDownRandomClustering
Creates a random top down clustering. Used for testing purposes.
#####
TransformedRegression
This meta learner applies a transformation on the label before the inner
 regression learner is applied.
#####
Tree2RuleConverter
This meta learner uses an inner tree learner and creates a rule model
 from the learned decision tree.
#####
UPGMAClustering
This operator generates a tree each node of which represents a cluster. UPGMA stands for Unweighted Pair Group Method using Arithmetic Means. Since
 the way cluster distances are calculated can be specified using parameters, this name is slightly misleading. Unfortunately, the name of the
 algorithm changes depending on the parameters used. <br/>Starting with initial clusters of size 1, the algorithm unites two clusters with minimal
 distance forming a new tree node. This is iterated until there is only one cluster left which forms the root of the tree. <br/>This operator does
 not generate a special cluster attribute and does not modify the input example set at all, since it generates too many clusters. The tree generated
 by this cluster is considered the interesting result of the algorithm.
#####
Vote
This class uses n+1 inner learners and generates n different models
 by using the last n learners. The predictions of these n models are
 taken to create n new features for the example set, which is finally
 used to serve as an input of the first inner learner.
#####
W-ADTree
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-AODE
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-AODEsr
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-AdaBoostM1
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-AdditiveRegression
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-Apriori
Performs the Weka association rule learner with the same name. The operator
 returns a result object containing the rules found by the association
 learner. In contrast to models generated by normal learners, the association
 rules cannot be applied to an example set. Hence, there is no way to evaluate
 the performance of association rules yet. See the Weka javadoc for further
 operator and parameter descriptions.
#####
W-BFTree
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-BIFReader
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-Bagging
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-BayesNet
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-BayesNetGenerator
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-BayesianLogisticRegression
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-CLOPE
This operator performs the Weka clustering scheme with the same name. The operator expects an example set containing ids and returns a
 FlatClusterModel or directly annotates the examples with a cluster attribute. Please note: Currently only clusterers that produce a partition of
 items are supported.
#####
W-CitationKNN
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-ClassBalancedND
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-ClassificationViaClustering
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-Cobweb
This operator performs the Weka clustering scheme with the same name. The operator expects an example set containing ids and returns a
 FlatClusterModel or directly annotates the examples with a cluster attribute. Please note: Currently only clusterers that produce a partition of
 items are supported.
#####
W-ComplementNaiveBayes
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-ConjunctiveRule
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-CostSensitiveClassifier
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-DTNB
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-Dagging
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-DataNearBalancedND
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-DecisionStump
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-DecisionTable
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-Decorate
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-EM
This operator performs the Weka clustering scheme with the same name. The operator expects an example set containing ids and returns a
 FlatClusterModel or directly annotates the examples with a cluster attribute. Please note: Currently only clusterers that produce a partition of
 items are supported.
#####
W-END
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-EditableBayesNet
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-EnsembleSelection
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-FLR
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-FT
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-FarthestFirst
This operator performs the Weka clustering scheme with the same name. The operator expects an example set containing ids and returns a
 FlatClusterModel or directly annotates the examples with a cluster attribute. Please note: Currently only clusterers that produce a partition of
 items are supported.
#####
W-FilteredClusterer
This operator performs the Weka clustering scheme with the same name. The operator expects an example set containing ids and returns a
 FlatClusterModel or directly annotates the examples with a cluster attribute. Please note: Currently only clusterers that produce a partition of
 items are supported.
#####
W-GaussianProcesses
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-GeneralizedSequentialPatterns
Performs the Weka association rule learner with the same name. The operator
 returns a result object containing the rules found by the association
 learner. In contrast to models generated by normal learners, the association
 rules cannot be applied to an example set. Hence, there is no way to evaluate
 the performance of association rules yet. See the Weka javadoc for further
 operator and parameter descriptions.
#####
W-Grading
Performs the ensemble learning scheme of Weka with the same name. An arbitrary
 number of other Weka learning schemes must be embedded as inner operators. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-GridSearch
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-HNB
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-HotSpot
Performs the Weka association rule learner with the same name. The operator
 returns a result object containing the rules found by the association
 learner. In contrast to models generated by normal learners, the association
 rules cannot be applied to an example set. Hence, there is no way to evaluate
 the performance of association rules yet. See the Weka javadoc for further
 operator and parameter descriptions.
#####
W-HyperPipes
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-IB1
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-IBk
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-Id3
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-IsotonicRegression
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-J48
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-J48graft
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-JRip
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-JythonClassifier
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-KStar
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-LBR
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-LMT
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-LWL
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-LeastMedSq
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-LinearRegression
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-Logistic
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-LogisticBase
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-LogitBoost
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-M5P
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-M5Rules
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MDD
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MIBoost
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MIDD
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MIEMDD
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MILR
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MINND
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MIOptimalBall
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MISMO
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MIWrapper
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MetaCost
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-MinMaxExtension
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-MultiBoostAB
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-MultiClassClassifier
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-MultiScheme
Performs the ensemble learning scheme of Weka with the same name. An arbitrary
 number of other Weka learning schemes must be embedded as inner operators. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-MultilayerPerceptron
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-NBTree
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-ND
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-NNge
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-NaiveBayes
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-NaiveBayesMultinomial
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-NaiveBayesMultinomialUpdateable
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-NaiveBayesSimple
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-NaiveBayesUpdateable
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-OLM
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-OSDL
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-OneR
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-OrdinalClassClassifier
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-PART
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-PLSClassifier
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-PaceRegression
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-PredictiveApriori
Performs the Weka association rule learner with the same name. The operator
 returns a result object containing the rules found by the association
 learner. In contrast to models generated by normal learners, the association
 rules cannot be applied to an example set. Hence, there is no way to evaluate
 the performance of association rules yet. See the Weka javadoc for further
 operator and parameter descriptions.
#####
W-Prism
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-RBFNetwork
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-REPTree
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-RacedIncrementalLogitBoost
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-RandomCommittee
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-RandomForest
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-RandomSubSpace
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-RandomTree
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-RegressionByDiscretization
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-Ridor
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-SMO
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-SMOreg
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-SVMreg
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-SerializedClassifier
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-SimpleCart
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-SimpleKMeans
This operator performs the Weka clustering scheme with the same name. The operator expects an example set containing ids and returns a
 FlatClusterModel or directly annotates the examples with a cluster attribute. Please note: Currently only clusterers that produce a partition of
 items are supported.
#####
W-SimpleLinearRegression
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-SimpleLogistic
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-SimpleMI
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-Stacking
Performs the ensemble learning scheme of Weka with the same name. An arbitrary
 number of other Weka learning schemes must be embedded as inner operators. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-StackingC
Performs the ensemble learning scheme of Weka with the same name. An arbitrary
 number of other Weka learning schemes must be embedded as inner operators. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-TLD
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-TLDSimple
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-Tertius
Performs the Weka association rule learner with the same name. The operator
 returns a result object containing the rules found by the association
 learner. In contrast to models generated by normal learners, the association
 rules cannot be applied to an example set. Hence, there is no way to evaluate
 the performance of association rules yet. See the Weka javadoc for further
 operator and parameter descriptions.
#####
W-ThresholdSelector
Performs the meta learning scheme of Weka with the same name. Another
 non-meta learning scheme of Weka must be embedded as inner operator. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-VFI
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-Vote
Performs the ensemble learning scheme of Weka with the same name. An arbitrary
 number of other Weka learning schemes must be embedded as inner operators. See the
 Weka javadoc for further classifier and parameter descriptions.<br/>
#####
W-VotedPerceptron
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-WAODE
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-Winnow
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-XMeans
This operator performs the Weka clustering scheme with the same name. The operator expects an example set containing ids and returns a
 FlatClusterModel or directly annotates the examples with a cluster attribute. Please note: Currently only clusterers that produce a partition of
 items are supported.
#####
W-ZeroR
Performs the Weka learning scheme with the same name. See the Weka javadoc
 for further classifier and parameter descriptions.<br/>
#####
W-sIB
This operator performs the Weka clustering scheme with the same name. The operator expects an example set containing ids and returns a
 FlatClusterModel or directly annotates the examples with a cluster attribute. Please note: Currently only clusterers that produce a partition of
 items are supported.
#####

AbsoluteSplitChain
<p>An operator chain that split an <i>ExampleSet</i> into two disjunct parts
 and applies the first child operator on the first part and applies the second
 child on the second part and the result of the first child. The total result
 is the result of the second operator.</p>
 
 <p>The input example set will be splitted based on a user defined absolute 
 numbers.</p>
#####
AverageBuilder
Collects all average vectors (e.g. PerformanceVectors) from the input and
 average those of the same type.
#####
ClusterIteration
This operator splits up the input example set according to the clusters and 
 applies its inner operators <var>number_of_clusters </var> time. 
 This requires the example set to have a special cluster attribute which 
 can be either created by a <i>Clusterer</i> or might be declared in the 
 attribute description file that was used when the data was loaded.
#####
EvolutionaryParameterOptimization
This operator finds the optimal values for a set of parameters using an evolutionary
 strategies approach which is often more appropriate than a grid search or a greedy search
 like the quadratic programming approach and leads to better results. The parameter 
 <var>parameters</var> is a list of key value pairs
 where the keys are of the form <code>operator_name.parameter_name</code> and
 the value for each parameter must be a semicolon separated pair of a minimum and a maximum value
 in squared parantheses, e.g. [10;100] for a range of 10 until 100. <br/> 
 The operator returns an
 optimal <i>ParameterSet</i> which can as well be written to a file with a
 <i>ParameterSetWriter</i>. This parameter set
 can be read in another process using a
 <i>ParameterSetLoader</i>. <br/> 
 The file format of the parameter set file is straightforward and can easily be
 generated by external applications. Each line is of the form 
 <center><code>operator_name.parameter_name = value</code></center> <br/> 
 Please refer to section
 <i>Advanced Processes/Parameter and performance analysis</i>
 for an example application.
#####
ExampleSetIterator
For each example set the ExampleSetIterator finds in its input, the inner
 operators are applied as if it was an OperatorChain. This operator can be
 used to conduct a process consecutively on a number of different data
 sets.
#####
ExperimentEmbedder
This operator can be used to embed a complete process definition into the current 
  process definition. 
  The process must have been written into a file before and will be loaded and 
  executed when the current process reaches this operator. Optionally, the input
  of this operator can be used as input for the embedded process. In both cases,
  the output of the process will be delivered as output of this operator. Please note
  that validation checks will not work for process containing an operator of this
  type since the check cannot be performed without actually loading the process.
#####
GridParameterOptimization
<p>This operator finds the optimal values for a set of parameters using a grid
 search. The parameter <var>parameters</var> is a list of key value pairs
 where the keys are of the form <code>operator_name.parameter_name</code> and
 the value is either a comma separated list of values (e.g. 10,15,20,25) or an 
 interval definition in the format [start;end;stepsize] (e.g. [10;25;5]).
 Alternatively a value grid pattern may be used by [e.g. [start;end;no_steps;scale],
 where scale identifies the type of the pattern.</p> 
 
 <p>The operator returns an
 optimal <i>ParameterSet</i> which can as well be written to a file with a
 <i>ParameterSetWriter</i>. This parameter set
 can be read in another process using a
 <i>ParameterSetLoader</i>.</p> 
 
 <p>The file format of the parameter set file is straightforward and can easily be
 generated by external applications. Each line is of the form
 <center><code>operator_name.parameter_name = value</code></center>
 </p>
   
 <p>Please refer to section
 <i>Advanced Processes/Parameter and performance analysis</i>
 for an example application. Another parameter optimization schems like the 
 <i>EvolutionaryParameterOptimizationOperator</i> might also be useful if the best ranges
 and dependencies are not known at all. Another operator which works similar to this parameter 
 optimization operator is the operator <i>ParameterIteration</i>. In contrast to the optimization
 operator, this operator simply iterates through all parameter combinations. This might be 
 especially useful for plotting purposes.
 </p>
#####
IteratingOperatorChain
Performs its inner operators for the defined number of times. The input of this
 operator will be the input of the first operator in the first iteration. 
 The output of each children operator is the input for the following one, the output
 of the last inner operator will be the input for the first child in the next iteration.
 The output of the last operator in the last iteration will be the output of this
 operator.
#####
LearningCurve
This operator first divides the input example set into two parts, a training set and a test set
 according to the parameter &quot;training_ratio&quot;. It then uses iteratively bigger subsets 
 from the fixed training set for learning (the first operator) and calculates the corresponding 
 performance values on the fixed test set (with the second operator).
#####
MultipleLabelIterator
Performs the inner operator for all label attributes, i.e. special attributes
 whose name starts with &quot;label&quot;. In each iteration one of the
 multiple labels is used as label. The results of the inner operators are
 collected and returned. The example set will be consumed during the
 iteration.
#####
OperatorEnabler
This operator can be used to enable and disable other operators. The operator which should 
  be enabled or disabled must be a child operator of this one. Together with one of the
  parameter optimizing or iterating operators this operator can be used to dynamically change
  the process setup which might be useful in order to test different layouts, e.g. the
  gain by using different preprocessing steps.
#####
OperatorSelector
This operator can be used to employ a single inner operator or operator chain.
  Which operator should be used can be defined by the parameter &quot;select_which&quot;.
  Together with one of the parameter optimizing or iterating operators this operator 
  can be used to dynamically change the process setup which might be useful in order to 
  test different layouts, e.g. the gain by using different preprocessing steps or chains
  or the quality of a certain learner.
#####
ParameterCloner
Sets a list of parameters using existing parameter values. <br/>
 
 The operator is similar to <i>ParameterSetter</i>, but differs from that in
 not requiring a ParameterSet input. It simply reads a parameter value from a
 source and uses it to set the parameter value of a target parameter. Both,
 source and target, are given in the format 'operator'.'parameter'. <br/>
 
 This operator is more general than ParameterSetter and could completely
 replace it. It is most useful, if you need a parameter which is optimized
 more than once within the optimization loop - ParameterSetter cannot be used
 here. <br/>
 
 These parameters can either be generated by a
 <i>ParameterOptimizationOperator</i> or read by a
 <i>ParameterSetLoader</i>. This operator is
 useful, e.g. in the following scenario. If one wants to find the best
 parameters for a certain learning scheme, one usually is also interested in
 the model generated with this parameters. While the first is easily possible
 using a <i>ParameterOptimizationOperator</i>, the latter is not possible
 because the <i>ParameterOptimizationOperator</i> does not return the
 IOObjects produced within, but only a parameter set. This is, because the
 parameter optimization operator knows nothing about models, but only about
 the performance vectors produced within. Producing performance vectors does
 not necessarily require a model. <br/> To solve this problem, one can use a
 <code>ParameterSetter</code>. Usually, a process definition with a
 <code>ParameterSetter</code> contains at least two operators of the same
 type, typically a learner. One learner may be an inner operator of the
 <i>ParameterOptimizationOperator</i> and may be named &quot;Learner&quot;,
 whereas a second learner of the same type named &quot;OptimalLearner&quot;
 follows the parameter optimization and should use the optimal parameter set
 found by the optimization. In order to make the <code>ParameterSetter</code>
 set the optimal parameters of the right operator, one must specify its name.
 Therefore, the parameter list <var>name_map</var> was introduced. Each
 parameter in this list maps the name of an operator that was used during
 optimization (in our case this is &quot;Learner&quot;) to an operator that
 should now use these parameters (in our case this is
 &quot;OptimalLearner&quot;).
#####
ParameterIteration
<p>In contrast to the <i>GridSearchParameterOptimizationOperator</i> operator this
 operators simply uses the defined parameters and perform the inner operators
 for all possible combinations. This can be especially usefull for plotting
 or logging purposes and sometimes also for simply configuring the parameters for 
 the inner operators as a sort of meta step (e.g. learning curve generation).</p>
 
 <p>This operator iterates through a set of parameters by using all possible
 parameter combinations. The parameter <var>parameters</var> is a list of key value pairs
 where the keys are of the form <code>operator_name.parameter_name</code> and
 the value is either a comma separated list of values (e.g. 10,15,20,25) or an 
 interval definition in the format [start;end;stepsize] (e.g. [10;25;5]). Additionally,
 the format [start;end;steps;scale] is allowed.</p>
 
 <p>Please note that this operator has two modes: synchronized and non-synchronized.
 In the latter, all parameter combinations are generated and the inner operators
 are applied for each combination. In the synchronized mode, no combinations are
 generated but the set of all pairs of the increasing number of parameters
 are used. For the iteration over a single parameter there is no difference
 between both modes. Please note that the number of parameter possibilities must be 
 the same for all parameters in the synchronized mode.</p>
#####
ParameterSetter
Sets a set of parameters. These parameters can either be generated by a
 <i>ParameterOptimizationOperator</i> or read by a
 <i>ParameterSetLoader</i>. This operator is
 useful, e.g. in the following scenario. If one wants to find the best
 parameters for a certain learning scheme, one usually is also interested in
 the model generated with this parameters. While the first is easily possible
 using a <i>ParameterOptimizationOperator</i>, the latter is not possible
 because the <i>ParameterOptimizationOperator</i> does not return the
 IOObjects produced within, but only a parameter set. This is, because the
 parameter optimization operator knows nothing about models, but only about
 the performance vectors produced within. Producing performance vectors does
 not necessarily require a model. <br/> To solve this problem, one can use a
 <code>ParameterSetter</code>. Usually, a process with a
 <code>ParameterSetter</code> contains at least two operators of the same
 type, typically a learner. One learner may be an inner operator of the
 <i>ParameterOptimizationOperator</i> and may be named &quot;Learner&quot;,
 whereas a second learner of the same type named &quot;OptimalLearner&quot;
 follows the parameter optimization and should use the optimal parameter set
 found by the optimization. In order to make the <code>ParameterSetter</code>
 set the optimal parameters of the right operator, one must specify its name.
 Therefore, the parameter list <var>name_map</var> was introduced. Each
 parameter in this list maps the name of an operator that was used during
 optimization (in our case this is &quot;Learner&quot;) to an operator that
 should now use these parameters (in our case this is
 &quot;OptimalLearner&quot;).
#####
PartialExampleSetLearner
This operator works similar to the <i>LearningCurveOperator</i>. 
 In contrast to this, it just splits the ExampleSet according to the
 parameter "fraction" and learns a model only on the subset. It can be used, 
 for example, in conjunction with <i>GridSearchParameterOptimizationOperator</i> 
 which sets the fraction parameter to values between 0 and 1. The advantage 
 is, that this operator can then be used inside of a <i>XValidation</i>,
 which delivers more stable result estimations.
#####
ProcessBranch
<p>This operator provides a conditional execution of parts of processes.
 It has to have two OperatorChains as childs. The first chain is processed
 if the specified condition is true, the second one is processed if it is false
 (if-then-else). The second chain may be omitted (if-then). In this case, this
 operator has only one inner operator.</p>
 
 <p>
 If the condition &quot;attribute_value_filter&quot; is used, the same attribute
 value conditions already known from the <i>ExampleFilter</i> operator can be used.
 In addition to the known attribute value relation format (e.g. &quot;att1&gt;=0.7&quot;),
 this operator expects an additional definition for the used example which cam be added in 
 &quot;[&quot; and &quot;]&quot; after the attribute value condition. The following values
 are possible:
 <ul>
 <li>a fixed number, e.g. &quot;att1&gt;0.7 [7]&quot; meaning that the value for attribute 
     &quot;att1&quot; for the example 7 must be greater than 0.7</li>
 <li>the wildcard &quot;*&quot; meaning that the attribute value condition must be
     fulfilled for all examples, e.g. &quot;att4&lt;=5 [*]&quot;</li>
 <li>no example definition, meaning the same as the wildcard definition [*]</li>
 </ul>
 </p>
#####
ProcessEmbedder
This operator can be used to embed a complete process definition into the current 
  process definition. 
  The process must have been written into a file before and will be loaded and 
  executed when the current process reaches this operator. Optionally, the input
  of this operator can be used as input for the embedded process. In both cases,
  the output of the process will be delivered as output of this operator. Please note
  that validation checks will not work for process containing an operator of this
  type since the check cannot be performed without actually loading the process.
#####
QuadraticParameterOptimization
This operator finds the optimal values for a set of parameters using a
 quadratic interaction model. The parameter <var>parameters</var> is a list
 of key value pairs where the keys are of the form
 <code>OperatorName.parameter_name</code> and the value is a comma
 separated list of values (as for the GridParameterOptimization operator). <br/> 
 The operator returns an optimal
 <i>ParameterSet</i> which can as well be written to a file with a
 <i>ParameterSetLoader</i>. This parameter set
 can be read in another process using an
 <i>ParameterSetLoader</i>. <br/> The file
 format of the parameter set file is straightforward and can also easily be
 generated by external applications. Each line is of the form 
 <center><code>operator_name.parameter_name = value</code></center>.
#####
RandomOptimizer
This operator iterates several times through the inner operators and in each
 cycle evaluates a performance measure. The IOObjects that are produced as
 output of the inner operators in the best cycle are then returned. The target
 of this operator are methods that involve some non-deterministic elements
 such that the performance in each cycle may vary. An example is k-means with
 random intialization.
#####
RepeatUntilOperatorChain
Performs its inner operators until all given criteria are met or a timeout
 occurs.
#####
SeriesPrediction
This operator can be used for some basic series prediction operations.
 The given series must be univariate and must be encoded by
 examples, i.e. each point of time is encoded by the values in one
 single example. The values which should be predicted must be defined
 by the label attribute. Other attributes will be ignored.
 
 The operator creates time windows and learns a model from these
 windows to predict the value of the label column after a certain amount
 of values (horizon). After predicting a value, the window is moved with step
 size 1 and the next value is predicted. All predictions are kept and can
 be compared afterwards to the actual values in a series plot or with a performance 
 evaluation operator.
 
 If you want predictions for different horizons, you have to restart
 this operator with different settings for horizon. This might be useful to
 get a prediction for 1 to horizon future time steps.
 
 The inner learner must be able to work on numerical regression problems.
#####
SplitChain
<p>An operator chain that split an <i>ExampleSet</i> into two disjunct parts
 and applies the first child operator on the first part and applies the second
 child on the second part and the result of the first child. The total result
 is the result of the second operator.</p>
 
 <p>The input example set will be splitted based on a defined ratio between 
 0 and 1.</p>
#####
XVPrediction
Operator chain that splits an <i>ExampleSet</i> into a training and test
 sets similar to XValidation, but returns the test set predictions instead of
 a performance vector. The inner two operators must be a learner returning a
 <i>Model</i> and an operator or operator chain that can apply this model
 (usually a model applier)
#####

ANOVAMatrix
<p>This operator calculates the significance of difference for the values for 
 all numerical attributes depending on the groups defined by all nominal attributes.
 Please refer to the operator <i>GroupedANOVAOperator</i> for details of the
 calculation.</p>
#####
Aggregation
<p>This operator creates a new example set from the input example set
 showing the result of the application of an arbitrary aggregation
 function (as SUM, COUNT etc. known from SQL). Before the values of 
 different rows are aggregated into a new row the rows might be
 grouped by the values of a single attribute (similar to the group-by
 clause known from SQL). In this case for each group a new line will
 be created.</p>
 
 <p>Please note that the known HAVING clause from SQL can be simulated
 by an additional <i>ExampleFilter</i> operator following this one.</p>
#####
GroupedANOVA
<p>This operator creates groups of the input example set based on
 the defined grouping attribute. For each of the groups the mean and
 variance of another attribute (the anova attribute) is calculated
 and an ANalysis Of VAriance (ANOVA) is performed. The result will
 be a significance test result for the specified significance level
 indicating if the values for the attribute are significantly different
 between the groups defined by the grouping attribute.</p>
#####

FrequentItemSetUnificator
This operator compares a number of FrequentItemSet sets and removes every
 not unique FrequentItemSet.
#####
PlattScaling
A scaling operator, applying the original algorithm by Platt (1999) to turn
 confidence scores of boolean classifiers into probability estimates.
 
 Unlike the original version this operator assumes that the confidence scores
 are already in the interval of [0,1], as e.g. given for the RapidMiner boosting
 operators. The crude estimates are then transformed into log odds, and scaled
 by the original transformation of Platt.
 
 The operator assumes a model and an example set for scaling. It outputs a
 PlattScalingModel, that contains both, the supplied model and the scaling
 step. If the example set contains a weight attribute, then this operator is
 able to fit a model to the weighted examples.
#####
ThresholdApplier
This operator applies the given threshold to an example set and maps a soft
 prediction to crisp values. If the confidence for the second class (usually
 positive for RapidMiner) is greater than the given threshold the prediction is set
 to this class.
#####
ThresholdCreator
This operator creates a user defined threshold for crisp classifying based on
 prediction confidences.
#####
ThresholdFinder
This operator finds the best threshold for crisp classifying based on user
 defined costs.
#####
UncertainPredictionsTransformation
This operator sets all predictions which do not have a higher confidence than the 
 specified one to &quot;unknown&quot; (missing value). This operator is a quite simple
 version of the CostBasedThresholdLearner which might be useful in simple binominal
 classification settings (although it does also work for polynominal classifications).
#####

AGA
Basically the same operator as the 
 <i>GeneratingGeneticAlgorithm</i> operator. 
 This version adds additional generators and improves the simple GGA approach by providing
 some basic intron prevention techniques. In general, this operator seems to work better than
 the original approach but frequently deliver inferior results compared to the operator
 <i>YAGGA2</i>.
#####
AbsoluteSampling
Absolute sampling operator. This operator takes a random sample with the
 given size. For example, if the sample size is set to 50, the result will
 have exactly 50 examples randomly drawn from the complete data set. Please
 note that this operator does not sample during a data scan but jumps to the
 rows. It should therefore only be used in case of memory data management and
 not, for example, for database or file management.
#####
AbsoluteStratifiedSampling
Stratified sampling operator. This operator performs a random sampling of a
 given size. In contrast to the simple sampling operator, this operator
 performs a stratified sampling for data sets with nominal label attributes,
 i.e. the class distributions remains (almost) the same after sampling. Hence,
 this operator cannot be applied on data sets without a label or with a
 numerical label. In these cases a simple sampling without stratification
 is performed. In some cases it might happen that not the exact desired number 
 of examples is sampled, e.g. if the desired number is 100 from three qually distributed
 classes the resulting number will be 99 (33 of each class).
#####
AddNominalValue
Adds a value to a nominal attribute definition.
#####
AttributeCopy
Adds a copy of a single attribute to the given example set.
#####
AttributeFilter
<p>This operator filters the attributes of an exampleSet. Therefore, different conditions may be selected as 
 parameter and only attributes fulfilling this condition are kept. The rest will be removed from the exampleSet
 The conditions may be inverted.
 The conditions are tested over all attributes and for every attribute over all examples. For example the
 numeric_value_filter with the parameter string &quot;&gt; 6&quot; will keep all nominal attributes and all numeric attributes
 having a value of greater 6 in every example. A combination of conditions is possible: &quot;&gt; 6 ANDAND &lt; 11&quot; or &quot;&lt;= 5 || &lt; 0&quot;.
 But ANDAND and || must not be mixed. Please note that ANDAND has to be replaced by two ampers ands.</p>

 <p>The attribute_name_filter keeps all attributes which names match the given regular expression.
 The nominal_value_filter keeps all numeric attribute and all nominal attributes containing at least one of specified 
 nominal values. &quot;rainy ANDAND cloudy&quot; would keep all attributes containing at least one time &quot;rainy&quot; and one time &quot;cloudy&quot;.
 &quot;rainy || sunny&quot; would keep all attributes containing at least one time &quot;rainy&quot; or one time &quot;sunny&quot;. ANDAND and || are not
 allowed to be mixed. And again, ANDAND has to be replaced by two ampers ands.</p>
#####
AttributeMerge
This operator merges two attributes by simply concatenating the values and store
 those new values in a new attribute which will be nominal. If the resulting values
 are actually numerical, you could simply change the value type afterwards with the
 corresponding operators.
#####
AttributeSubsetPreprocessing
<p>This operator can be used to select one attribute (or a subset) by defining a 
 regular expression for the attribute name and applies its inner operators to
 the resulting subset. Please note that this operator will also use special 
 attributes which makes it necessary for all preprocessing steps which should
 be performed on special attributes (and are normally not performed on special
 attributes).</p>
 
 <p>This operator is also able to deliver the additional results of the inner
 operator if desired.</p> 
 
 <p>Afterwards, the remaining original attributes are added
 to the resulting example set if the parameter &quot;keep_subset_only&quot; is set to 
 false (default).</p>
 
 <p>Please note that this operator is very powerful and can be used to create
 new preprocessing schemes by combinating it with other preprocessing operators.
 Hoewever, there are two major restrictions (among some others): first, since the inner result
 will be combined with the rest of the input example set, the number of 
 examples (data points) is not allowed to be changed inside of the subset preprocessing. 
 Second, attribute role changes will not be delivered to the outside since internally all special
 attributes will be changed to regular for the inner operators and role changes can afterwards
 not be delivered.</p>
#####
AttributeValueMapper
<p>This operator takes an <code>ExampleSet</code> as input and maps the values
 of certain attributes to other values. For example, it can replace all
 occurrences of the String "unknown" in a nominal Attribute by a default
 String, for all examples in the ExampleSet.</p>
 
 <p>This operator can replace nominal values (e.g. replace the value &quot;green&quot; by 
 the value &quot;green_color&quot;) as well as numerical values (e.g. replace the all values
 &quot;3&quot; by &quot;-1&quot;).</p>
 
 <p> This operator supports regular expressions for the attribute names, i.e. the value
 mapping is applied on all attributes for which the name fulfills the pattern defined
 by the name expression.</p>
#####
AttributeWeightSelection
This operator selects all attributes which have a weight fulfilling a given
 condition. For example, only attributes with a weight greater than
 <code>min_weight</code> should be selected. This operator is also able
 to select the k attributes with the highest weight.
#####
AttributeWeightsApplier
<p>This operator deselects attributes with a weight value of 0.0. The values of
 the other numeric attributes will be recalculated based on the
 weights delivered as <i>AttributeWeights</i>
 object in the input.</p>
 
 <p>This operator can hardly be used to select a subset of
 features according to weights determined by a former weighting scheme. For this purpose
 the operator
 <i>AttributeWeightSelection</i>
 should be used which will select only those attribute fulfilling a specified 
 weight relation.</p>
#####
Attributes2RealValues
This operator maps all non numeric attributes to real valued attributes.
 Nothing is done for numeric attributes, binary attributes are mapped to 0 and
 1.
 
 For nominal attributes one of the following calculations will be done:
 <ul>
 <li>Dichotomization, i.e. one new attribute for each value of the nominal
 attribute. The new attribute which corresponds to the actual nominal value
 gets value 1 and all other attributes gets value 0.</li>
 <li>Alternatively the values of nominal attributes can be seen as equally
 ranked, therefore the nominal attribute will simply be turned into a real
 valued attribute, the old values results in equidistant real values.</li>
 </ul>
 
 At this moment the same applies for ordinal attributes, in a future release
 more appropriate values based on the ranking between the ordinal values may
 be included.
#####
BackwardWeighting
Uses the backward selection idea for the weighting of features.
#####
BinDiscretization
This operator discretizes all numeric attributes in the dataset into nominal attributes. This discretization is performed by simple binning, i.e. the specified number of equally sized bins is created and the numerical values are simply sorted into
 those bins. Skips all special attributes including the label.
#####
Bootstrapping
This operator constructs a bootstrapped sample from the given example set. That means
 that a sampling with replacement will be performed. The usual sample size is the number 
 of original examples. This operator also offers the possibility to create the inverse
 example set, i.e. an example set containing all examples which are not part of the 
 bootstrapped example set. This inverse example set might be used for a bootstrapped
 validation (together with an <i>IteratingPerformanceAverage</i> operator.
#####
BruteForce
This feature selection operator selects the best attribute set by trying all
 possible combinations of attribute selections. It returns the example set
 containing the subset of attributes which produced the best performance. As
 this operator works on the powerset of the attributes set it has exponential
 runtime.
#####
ChangeAttributeName
<p>
 This operator can be used to rename an attribute of the input example set.
 If you want to change the attribute type (e.g. from regular to id attribute or from label to regular etc.),
 you should use the <i>ChangeAttributeType</i> operator.
 </p>
#####
ChangeAttributeRole
<p>
 This operator can be used to change the attribute type of an attribute of the input example set.
 If you want to change the attribute name you should use the <i>ChangeAttributeName</i> operator.
 </p>

 <p>
 The target type indicates if the attribute is a regular attribute (used by learning operators) or a
 special attribute (e.g. a label or id attribute). The following target
 attribute types are possible:
 </p>
 <ul>
 <li>regular: only regular attributes are used as input variables for learning tasks</li>
 <li>id: the id attribute for the example set</li>
 <li>label: target attribute for learning</li>
 <li>prediction: predicted attribute, i.e. the predictions of a learning scheme</li>
 <li>cluster: indicates the memebership to a cluster</li>
 <li>weight: indicates the weight of the example</li>
 <li>batch: indicates the membership to an example batch</li>
 </ul>
 <p>
 Users can also define own attribute types by simply using the desired name.
 </p>
#####
ChangeAttributeType
<p>
 This operator can be used to change the attribute type of an attribute of the input example set.
 If you want to change the attribute name you should use the <i>ChangeAttributeName</i> operator.
 </p>

 <p>
 The target type indicates if the attribute is a regular attribute (used by learning operators) or a
 special attribute (e.g. a label or id attribute). The following target
 attribute types are possible:
 </p>
 <ul>
 <li>regular: only regular attributes are used as input variables for learning tasks</li>
 <li>id: the id attribute for the example set</li>
 <li>label: target attribute for learning</li>
 <li>prediction: predicted attribute, i.e. the predictions of a learning scheme</li>
 <li>cluster: indicates the memebership to a cluster</li>
 <li>weight: indicates the weight of the example</li>
 <li>batch: indicates the membership to an example batch</li>
 </ul>
 <p>
 Users can also define own attribute types by simply using the desired name.
 </p>
#####
ChiSquaredWeighting
This operator calculates the relevance of a feature by computing 
 for each attribute of the input example set the value of the 
 chi-squared statistic with respect to the class attribute.
#####
CompleteFeatureGeneration
This operator applies a set of functions on all features of the input example
 set. Applicable functions include +, -, *, /, norm, sin, cos, tan, atan, exp,
 log, min, max, floor, ceil, round, sqrt, abs, and pow. Features with two
 arguments will be applied on all pairs. Non commutative functions will also
 be applied on all permutations.
#####
ComponentWeights
For models creating components like <code>PCA</code>, <code>GHA</code>
 and <code>FastICA</code> you can create the <code>AttributeWeights</code>
 from a component.
#####
CorpusBasedWeighting
This operator uses a corpus of examples to characterize a single class by
 setting feature weights. Characteristic features receive higher weights than
 less characteristic features. The weight for a feature is determined by
 calculating the average value of this feature for all examples of the target
 class. This operator assumes that the feature values characterize the
 importance of this feature for an example (e.g. TFIDF or others). Therefore,
 this operator is mainly used on textual data based on TFIDF weighting
 schemes. To extract such feature values from text collections you can use the
 Word Vector Tool plugin.
#####
DeObfuscator
This operator takes an <code>ExampleSet</code> as input and maps all
 nominal values to randomly created strings. The names and the construction
 descriptions of all attributes will also replaced by random strings. This
 operator can be used to anonymize your data. It is possible to save the
 obfuscating map into a file which can be used to remap the old values and
 names. Please use the operator <code>Deobfuscator</code> for this purpose.
 The new example set can be written with an <code>ExampleSetWriter</code>.
#####
DensityBasedOutlierDetection
<p>This operator is a DB outlier detection algorithm which calculates 
 the DB(p,D)-outliers for an ExampleSet passed to the operator.
 DB(p,D)-outliers are Distance based outliers according to Knorr and Ng. 
 A DB(p,D)-outlier is an object to which at least a proportion of p of all 
 objects are farer away than distance D. It implements a global homogenous 
 outlier search.</p>
 
 <p>Currently, the operator supports cosine, sine or squared distances in addition
 to the usual euclidian distance which can be specified by the corresponding parameter.
 The operator takes two other real-valued parameters p and D. Depending on these 
 parameters, search objects will be created from the examples in the ExampleSet 
 passed to the operator. These search objects will be added to a search space which 
 will perform the outlier search according to the DB(p,D) scheme.</p>
 
 <p>The Outlier status (boolean in its nature) is written to a new special attribute
 &quot;Outlier&quot; and is passed on with the example set.</p>
#####
DistanceBasedOutlierDetection
<p>This operator performs a D^k_n Outlier Search according to the outlier detection 
 approach recommended by Ramaswamy, Rastogi and Shim in "Efficient Algorithms for 
 Mining Outliers from Large Data Sets". It is primarily a statistical outlier search
 based on a distance measure similar to the DB(p,D)-Outlier Search from Knorr and Ng. 
 But it utilizes a distance search through the k-th nearest neighbourhood, so it 
 implements some sort of locality as well.</p>
 
 <p>The method states, that those objects with the largest distance to their k-th nearest 
 neighbours are likely to be outliers respective to the data set, because it can be assumed, 
 that those objects have a more sparse neighbourhood than the average objects. As this 
 effectively provides a simple ranking over all the objects in the data set according to 
 the distance to their k-th nearest neighbours, the user can specify a number of n objects 
 to be the top-n outliers in the data set.</p>
 
 <p>The operator supports cosine, sine or squared distances in addition to the euclidian 
 distance which can be specified by a distance parameter. The Operator takes an example set
 and passes it on with an boolean top-n D^k outlier status in a new boolean-valued
 special outlier attribute indicating true (outlier) and false (no outlier).</p>
#####
EvolutionaryFeatureAggregation
Performs an evolutionary feature aggregation. Each base feature is only
 allowed to be used as base feature, in one merged feature, or it may not be
 used at all.
#####
EvolutionaryWeighting
This operator performs the weighting of features with an evolutionary
 strategies approach. The variance of the gaussian additive mutation can be
 adapted by a 1/5-rule.
#####
ExampleFilter
This operator takes an <i>ExampleSet</i> as input and returns a new
 <i>ExampleSet</i> including only the <i>Example</i>s that fulfill a
 condition. <br/> By specifying an implementation of
 <i>Condition</i> and a parameter string, arbitrary
 filters can be applied. Users can implement their own conditions by writing a
 subclass of the above class and implementing a two argument constructor
 taking an <i>ExampleSet</i> and a parameter string. This parameter string is
 specified by the parameter <code>parameter_string</code>. Instead of using
 one of the predefined conditions users can define their own implementation
 with the fully qualified class name. <br/> For
 &quot;attribute_value_condition&quot; the parameter string must have the form
 <code>attribute op value</code>, where attribute is a name of an
 attribute, value is a value the attribute can take and op is one of the
 binary logical operators similar to the ones known from Java, e.g. greater
 than or equals. <br/> For &quot;unknown_attributes&quot; the parameter string
 must be empty. This filter removes all examples containing attributes that
 have missing or illegal values. For &quot;unknown_label&quot; the parameter
 string must also be empty. This filter removes all examples with an unknown
 label value.
#####
ExampleRangeFilter
This operator keeps only the examples of a given range (including the borders). 
 The other examples will be removed from the input example set.
#####
ExampleSet2AttributeWeights
This operator creates a new attribute weights IOObject from a given example
 set. The result is a vector of attribute weights containing the weight 1.0
 for each of the input attributes.
#####
ExampleSetCartesian
<p>Build the cartesian product of two example sets. In contrast to the <i>ExampleSetJoin</i>
  operator, this operator does not depend on Id attributes. The result example set will
  consist of the union set or the union list (depending on parameter
  setting double attributes will be removed or renamed) of both feature sets. In case of removing 
  double attribute the attribute values must be the same for the examples of both example set, otherwise
  an exception will be thrown.</p>
 
  <p>Please note that this check for double attributes will only be applied for regular attributes. 
  Special attributes of the second input example set which do not exist in the first example set will
  simply be added. If they already exist they are simply skipped.</p>
#####
ExampleSetJoin
<p>
 Build the join of two example sets using the id attributes of the sets, i.e. both example sets must have an id attribute where the same id indicate the same examples. If examples are missing an
 exception will be thrown. The result example set will consist of the same number of examples but the union set or the union list (depending on parameter setting double attributes will be removed or
 renamed) of both feature sets. In case of removing double attribute the attribute values must be the same for the examples of both example set, otherwise an exception will be thrown.
 </p>
 <p>
 Please note that this check for double attributes will only be applied for regular attributes. Special attributes of the second input example set which do not exist in the first example set will
 simply be added. If they already exist they are simply skipped.
 </p>
#####
ExampleSetMerge
<p>This operator merges two or more given example sets by adding all examples in 
 one example table containing all data rows. Please note that the new example table
 is built in memory and this operator might therefore not be applicable for merging
 huge data set tables from a database. In that case other preprocessing tools should
 be used which aggregates, joins, and merges tables into one table which is then used
 by RapidMiner.</p>
 
 <p>All input example sets must provide the same attribute signature. That means that
 all examples sets must have the same number of (special) attributes and attribute names.
 If this is true this operator simply merges all example sets by adding all examples of all
 table into a new set which is then returned.</p>
#####
ExampleSetTranspose
<p>This operator transposes an example set, i.e. the columns with become the
 new rows and the old rows will become the columns. Hence, this operator
 works very similar to the well know transpose operation for matrices.</p> 
 
 <p>If an Id attribute is part of the given example set, the ids will become 
 the names of the new attributes. The names of the old attributes will be 
 transformed into the id values of a new special Id attribute. Since no 
 other &quot;special&quot; examples or data rows exist, all other new 
 attributes will be regular after the transformation. You can use 
 the <i>ChangeAttributeType</i> operator in order to change one of
 these into a special type afterwards.</p>
 
 <p>If all old attribute have the same value type, all new attributes
 will have this value type. Otherwise, the new value types will all be
 &quot;nominal&quot; if at least one nominal attribute was part of the 
 given example set and &quot;real&quot; if the types contained mixed 
 numbers.</p>
 
 <p>This operator produces a copy of the data in the main memory and it 
 therefore not suggested to use it on very large data sets.</p>
#####
ExchangeAttributeRoles
This operator changes the attribute roles of two input attributes. This could for 
 example be useful to exchange the roles of a label with a regular attribute (and 
 vice versa), or a label with a batch attribute, a label with a cluster etc.
#####
FastICA
This operator performs the independent componente analysis (ICA).
 Implementation of the FastICA-algorithm of Hyvaerinen und Oja. The operator
 outputs a <code>FastICAModel</code>. With the <code>ModelApplier</code>
 you can transform the features.
#####
FeatureBlockTypeFilter
This operator switches off all features whose block type matches the one
 given in the parameter <code>skip_features_of_type</code>. This can be
 useful e.g. for preprocessing operators that can handle only series
 attributes.
#####
FeatureGeneration
This operator generates new user specified features. The new features are
 specified by their function names (prefix notation) and their arguments using
 the names of existing features.<br/> Legal function names include +, -, *, /,
 norm, sin, cos, tan, atan, exp, log, min, max, floor, ceil, round, sqrt, abs,
 and pow. Constant values can be defined by &quot;const[value]()&quot; where
 value is the desired value. Do not forget the empty round brackets. Example:
 <code>+(a1, *(a2, a3))</code> will calculate the sum of the attribute
 <code>a1</code> and the product of the attributes <code>a2</code> and
 <code>a3</code>. <br/> Features are generated in the following order
 <ol>
 <li>Features specified by the file referenced by the parameter "filename"
 are generated</li>
 <li>Features specified by the parameter list "functions" are generated</li>
 <li>If "keep_all" is false, all of the old attributes are removed now</li>
 </ol>
#####
FeatureNameFilter
This operator switches off all features whose name matches the one given in
 the parameter <code>skip_features_with_name</code>. The name can be
 defined as a regular expression.
#####
FeatureRangeRemoval
This operator removes the attributes of a given range. The first and last
 attribute of the range will be removed, too. Counting starts with 1.
#####
FeatureSelection
<p>
 This operator realizes the two deterministic greedy feature selection
 algorithms forward selection and backward elimination. However, we added some
 enhancements to the standard algorithms which are described below:
 </p>
 
 <h4>Forward Selection</h4>
 <ol>
 <li>Create an initial population with <i>n</i> individuals where
 <i>n</i> is the input example set's number of attributes. Each
 individual will use exactly one of the features.</li>
 <li>Evaluate the attribute sets and select only the best <i>k</i>.</li>
 <li>For each of the <i>k</i> attribute sets do: If there are
 <i>j</i> unused attributes, make <i>j</i> copies of the attribute
 set and add exactly one of the previously unused attributes to the attribute
 set.</li>
 <li>As long as the performance improved in the last <i>p</i>
 iterations go to 2</li>
 </ol>
 
 <h4>Backward Elimination</h4>
 <ol>
 <li>Start with an attribute set which uses all features.</li>
 <li>Evaluate all attribute sets and select the best <i>k</i>.</li>
 <li>For each of the <i>k</i> attribute sets do: If there are
 <i>j</i> attributes used, make <i>j</i> copies of the attribute
 set and remove exactly one of the previously used attributes from the
 attribute set.</li>
 <li>As long as the performance improved in the last <i>p</i>
 iterations go to 2</li>
 </ol>
 
 <p>
 The parameter <i>k</i> can be specified by the parameter
 <code>keep_best</code>, the parameter <i>p</i> can be specified by
 the parameter <code>generations_without_improval</code>. These parameters
 have default values 1 which means that the standard selection algorithms are
 used. Using other values increase the runtime but might help to avoid local
 extrema in the search for the global optimum.
 </p>
 
 <p>
 Another unusual parameter is <code>maximum_number_of_generations</code>.
 This parameter bounds the number of iterations to this maximum of feature
 selections / deselections. In combination with
 <code>generations_without_improval</code> this allows several different
 selection schemes (which are described for forward selection, backward
 elimination works analogous):
 
 <ul>
 <li><code>maximum_number_of_generations</code> = <i>m</i> and
 <code>generations_without_improval</code> = <i>p</i>: Selects
 maximal <i>m</i> features. The selection stops if not performance
 improvement was measured in the last <i>p</i> generations.</li>
 <li><code>maximum_number_of_generations</code> = <i>-1</i> and
 <code>generations_without_improval</code> = <i>p</i>: Tries to
 selects new features until no performance improvement was measured in the
 last <i>p</i> generations.</li>
 <li><code>maximum_number_of_generations</code> = <i>m</i> and
 <code>generations_without_improval</code> = <i>-1</i>: Selects
 maximal <i>m</i> features. The selection stops is not stopped until all
 combinations with maximal <i>m</i> were tried. However, the result
 might contain less features than these.</li>
 <li><code>maximum_number_of_generations</code> = <i>-1</i> and
 <code>generations_without_improval</code> = <i>-1</i>: Test all
 combinations of attributes (brute force, this might take a very long time and
 should only be applied to small attribute sets).</li>
 </ul>
 </p>
#####
FeatureValueTypeFilter
This operator switches off all features whose value type matches the one
 given in the parameter <code>skip_features_of_type</code>. This can be
 useful e.g. for learning schemes that can handle only nominal attributes.
#####
ForwardWeighting
This operator performs the weighting under the naive assumption that the
 features are independent from each other. Each attribute is weighted with a
 linear search. This approach may deliver good results after short time if the
 features indeed are not highly correlated.
#####
FourierTransform
Creates a new example set consisting of the result of a fourier
 transformation for each attribute of the input example set.
#####
FrequencyDiscretization
This operator discretizes all numeric attributes in the dataset into nominal attributes. This discretization is performed by equal frequency binning, i.e. the thresholds of all bins is selected in a way that all bins contain the same number of
 numerical values. The number of bins is specified by a parameter, or, alternatively, is calculated as the square root of the number of examples with non-missing values (calculated for every single attribute). Skips all special attributes including
 the label.
#####
FrequentItemSetAttributeCreator
This operator takes all FrequentItemSet sets within IOObjects and
 creates attributes for every frequent item set. This attributes indicate
 if the examples contains all values of this frequent item set. The attributes
 will contain values 0 or 1 and are numerical.
#####
FunctionValueSeries
Calculates for each sample a series of function values. Therefore, the weights
 of the given <code>JMySVMModel</code> are ordered descending by their absolute
 value. The x-th value of the series is the function valueof the example by
 taking the first x weights. The other weights are to zero. So the series
 attribute count values. Additionally the user can enter nr_attributes which
 are summerized to one value calculation. This can reduce the number of
 calculations dramatically. The result is an <code>ExampleSet</code>
 containing for each example a series of function values given by the
 attribute values.
#####
GHA
Generalized Hebbian Algorithm (GHA) is an iterative method to compute
 principal components. From a computational point of view, it can be
 advantageous to solve the eigenvalue problem by iterative methods which do
 not need to compute the covariance matrix directly. This is useful when the
 ExampleSet contains many Attributes (hundreds, thousands). The operator
 outputs a <code>GHAModel</code>. With the <code>ModelApplier</code> you
 can transform the features.
#####
GeneratingForwardSelection
This operator is a kind of nested forward selection and thus is (in contrast
 to a genetic algorithm) a directed search.
 <ol>
 <li>use forward selection in order to determine the best attributes</li>
 <li>Create a new attribute by multiplying any of the original attributes
 with any of the attributes selected by the forward selection in the last turn</li>
 <li>loop as long as performance increases</li>
 </ol>
#####
GeneratingGeneticAlgorithm
In contrast to the class
 <i>GeneticAlgorithm</i>, the
 <i>GeneratingGeneticAlgorithm</i> generates new attributes and thus can
 change the length of an individual. Therfore specialized mutation and
 crossover operators are being applied. Generators are chosen at random from a
 list of generators specified by boolean parameters. <br/>
 
 Since this operator does not contain algorithms to extract features from
 value series, it is restricted to example sets with only single attributes.
 For automatic feature extraction from values series the value series plugin
 for RapidMiner written by Ingo Mierswa should be used. It is available at <a
 href="http://rapid-i.com">http://rapid-i.com</a>
#####
GeneticAlgorithm
A genetic algorithm for feature selection (mutation=switch features on and
 off, crossover=interchange used features). Selection is done by roulette
 wheel. Genetic algorithms are general purpose optimization / search
 algorithms that are suitable in case of no or little problem knowledge. <br/>
 
 A genetic algorithm works as follows
 <ol>
 <li>Generate an initial population consisting of
 <code>population_size</code> individuals. Each attribute is switched on
 with probability <code>p_initialize</code></li>
 <li>For all individuals in the population
 <ul>
 <li>Perform mutation, i.e. set used attributes to unused with probability
 <code>p_mutation</code> and vice versa.</li>
 <li>Choose two individuals from the population and perform crossover with
 probability <code>p_crossover</code>. The type of crossover can be
 selected by <code>crossover_type</code>.</li>
 </ul>
 </li>
 <li>Perform selection, map all individuals to sections on a roulette wheel
 whose size is proportional to the individual's fitness and draw
 <code>population_size</code> individuals at random according to their
 probability.</li>
 <li>As long as the fitness improves, go to 2</li>
 </ol>
 
 If the example set contains value series attributes with blocknumbers, the
 whole block will be switched on and off.
#####
GiniIndexWeighting
This operator calculates the relevance of a feature by computing the 
 Gini index of the class distribution, if the given example set would 
 have been splitted according to the feature.
#####
GroupBy
<p>This operator creates a SplittedExampleSet from an arbitrary example set. 
 The partitions of the resulting example set are created according to the 
 values of the specified attribute. This works similar to the 
 <code>GROUP BY</code> clause in SQL.</p>
 
 <p>Please note that the resulting example set is simply a splitted example
 set where no subset is selected. Following operators might decide to select
 one or several of the subsets, e.g. one of the aggregation operators.</p>
#####
GuessValueTypes
This operator can be used to (re-)guess the value types of all attributes. This
 might be useful after some preprocessing transformations and &quot;purifying&quot;
 some of the columns, especially if columns which were nominal before can be handled
 as numerical columns. With this operator, the value types of all attributes do not
 have to be transformed manually with operators like <i>NominalNumbers2Numerical</i>.
#####
HyperplaneProjection
Projects the examples onto the hyperplane using AttributeWeights as the
 normal. Additionally the user can specify a bias of the hyperplane.
#####
IdTagging
This operator adds an ID attribute to the given example set. Each example is
 tagged with an incremental integer number. If the example set already
 contains an id attribute, the old attribute will be removed before the new
 one is added.
#####
InfiniteValueReplenishment
Replaces positive and negative infinite values in examples by one of the
 functions &quot;none&quot;, &quot;zero&quot;, &quot;max_byte&quot;,
 &quot;max_int&quot;, &quot;max_double&quot;, and &quot;missing&quot;.
 &quot;none&quot; means, that the value is not replaced. The max_xxx functions
 replace plus infinity by the upper bound and minus infinity by the lower
 bound of the range of the Java type xxx. &quot;missing&quot; means, that the
 value is replaced by nan (not a number), which is internally used to
 represent missing values. A <i>MissingValueReplenishment</i> operator can be
 used to replace missing values by average (or the mode for nominal
 attributes), maximum, minimum etc. afterwards.<br/> For each attribute, the
 function can be selected using the parameter list <code>columns</code>. If
 an attribute's name appears in this list as a key, the value is used as the
 function name. If the attribute's name is not in the list, the function
 specified by the <code>default</code> parameter is used.
#####
InfoGainRatioWeighting
This operator calculates the relevance of a feature by computing the 
 information gain ratio for the class distribution (if exampleSet would 
 have been splitted according to each of the given features).
#####
InfoGainWeighting
This operator calculates the relevance of a feature by computing the information gain in class distribution, if exampleSet would be splitted after
 the feature.
#####
InteractiveAttributeWeighting
This operator shows a window with the currently used attribute weights and
 allows users to change the weight interactively.
#####
IterativeWeightOptimization
Performs an iterative feature selection guided by the AttributeWeights. Its
 an backward feature elemination where the feature with the smallest weight
 value is removed. After each iteration the weight values are updated (e.g. by
 a learner like JMySVMLearner).
#####
KennardStoneSampling
This operator performs a Kennard-Stone Sampling. This sampling Algorithm works as follows:
 First find the two points most separated in the training set.
 For each candidate point, find the smallest distance to any object already selected.
 Select that point for the training set which has the largest of these smallest distances
 As described above, this algorithm always gives the same result, due to the two starting 
 points which are always the same.
 This implementation reduces number of iterations by holding a list with candidates of the largest
 smallest distances. 
 The parameters controll the number of examples in the sample
#####
LOFOutlierDetection
<p>This operator performs a LOF outlier search. LOF outliers or outliers with a local 
 outlier factor per object are density based outliers according to Breuning, 
 Kriegel, et al.</p>
 
 <p>The approach to find those outliers is based on measuring the density of objects 
 and its relation to each other (referred to as local reachability density). 
 Based on the average ratio of the local reachability density of an object and its
 k-nearest neighbours (e.g. the objects in its k-distance neighbourhood), a local 
 outlier factor (LOF) is computed. The approach takes a parameter MinPts 
 (actually specifying the "k") and it uses the maximum LOFs for objects in a MinPts range
 (lower bound and upper bound to MinPts).</p>
 
 <p>Currently, the operator supports cosine, sine or squared distances in addition
 to the usual euclidian distance which can be specified by the corresponding parameter.
 In the first step, the objects are grouped into containers. For each object, using a 
 radius screening of all other objects, all the available distances between that object 
 and another object (or group of objects) on the (same) radius given by the
 distance are associated with a container. That container than has the distance 
 information as well as the list of objects within that distance (usually only a few) 
 and the information, how many objects are in the container.</p>
 
 <p>In the second step, three things are done: (1) The containers for each object are counted 
 in acending order according to the cardinality of the object list within the container 
 (= that distance) to find the k-distances for each object and the
 objects in that k-distance (all objects in all the subsequent containers with a smaller 
 distance). (2) Using this information, the local reachability densities are computed by 
 using the maximum of the actual distance and the k-distance for each
 object pair (object and objects in k-distance) and averaging it by the cardinality of 
 the k-neighbourhood and than taking the reciprocal value. (3) The LOF is computed for 
 each MinPts value in the range (actually for all up to upper bound) by
 averaging the ratio between the MinPts-local reachability-density of all objects in 
 the k-neighbourhood and the object itself. The maximum LOF in the MinPts range is 
 passed as final LOF to each object.</p>
 
 <p>Afterwards LOFs are added as values for a special real-valued outlier attribute 
 in the example set which the operator will return.</p>
#####
LabelTrend2Classification
<p>
 This operator iterates over an example set with numeric label and converts the 
 label values to either the class 'up' or the class 'down' based on whether the 
 change from the previous label is positive or negative. Please note that this 
 does not make sense on example sets where the examples are not ordered in some 
 sense (like, e.g. ordered according to time). This operator might become useful
 in the context of a <i>Series2WindowExamples</i> operator. 
 </p>
#####
LinearCombination
<p>This operator applies a linear combination for each vector of the input ExampleSet, i.e.
 it creates a new feature containing the sum of all values of each row.</p>
#####
MergeNominalValues
Merges two nominal values of a given attribute.
#####
MinimalEntropyPartitioning
<p>This operator discretizes all numeric attributes in the dataset into nominal attributes. The discretization is performed by selecting a bin boundary minimizing the entropy in the induced partitions. The method is then applied recursively for both
 new partitions until the stopping criterion is reached. For Details see a) Multi-interval discretization of continued-values attributes for classification learning (Fayyad,Irani) and b) Supervised and Unsupervised
 Discretization  (Dougherty,Kohavi,Sahami). Skips all special attributes including the label.</p>
 
 <p>
 Please note that this operator automatically removes all attributes with only one range (i.e. those attributes which
 are not actually discretized since the entropy criterion is not fulfilled). This behavior can be controlled
 by the remove_useless parameter. 
 </p>
#####
MissingValueImputation
The operator MissingValueImpution imputes missing values by learning models
 for each attribute (except the label) and applying those models to the data
 set. The learner which is to be applied has to be given as inner operator.
 In order to specify a subset of the example set in which the missing values
 should be imputed (e.g. to limit the imputation to only numerical attributes) an
 arbitrary filter can be used as the first inner operator. In the case that such
 a filter is used, the learner has to be the second inner operator.
 
 Please be aware that depending on the ability of the inner operator to handle
 missing values this operator might not be able to impute all missing values in
 some cases. This behaviour leads to a warning. It might hence be useful to combine
 this operator with a subsequent MissingValueReplenishment.
 
 ATTENTION: This operator is currently under development and does not properly
 work in all cases. We do not recommend the usage of this operator in production 
 systems.
#####
MissingValueReplenishment
Replaces missing values in examples. If a value is missing, it is replaced by
 one of the functions &quot;minimum&quot;, &quot;maximum&quot;,
 &quot;average&quot;, and &quot;none&quot;, which is applied to the non
 missing attribute values of the example set. &quot;none&quot; means, that the
 value is not replaced. The function can be selected using the parameter list
 <code>columns</code>. If an attribute's name appears in this list as a
 key, the value is used as the function name. If the attribute's name is not
 in the list, the function specified by the <code>default</code> parameter
 is used. For nominal attributes the mode is used for the average, i.e. the
 nominal value which occurs most often in the data. For nominal attributes and
 replacement type zero the first nominal value defined for this
 attribute is used. The replenishment &quot;value&quot; indicates that the 
 user defined parameter should be used for the replacement.
#####
MissingValueReplenishmentView
This operator simply creates a new view on the input data without
 changing the actual data or creating a new data table. The new view 
 will not contain any missing values for regular attributes but 
 the mean value (or mode) of the non-missing values instead.
#####
ModelBasedSampling
Sampling based on a learned model.
#####
MultivariateSeries2WindowExamples
<p>This operator transforms a given example set containing series data into a
 new example set containing single valued examples. For this purpose, windows with
 a specified window and step size are moved across the series and the attribute
 value lying horizon values after the window end is used as label which should
 be predicted. In contrast to the <i>Series2WindowExamples</i> operator, this operator
 can also handle multivariate series data. In order to specify the dimension which should
 be predicted, one must use the parameter &quot;label_dimension&quot; (counting starts at
 0). If you want to predict all dimensions of your multivariate series you 
 must setup several process definitions with different label dimensions, one for each dimension.</p>
 
 <p>
 The series data must be given as ExampleSet. The parameter &quot;series_representation&quot;
 defines how the series data is represented by the ExampleSet:</p>
 <ul>
 <li>encode_series_by_examples</li>: the series index variable (e.g. time) is encoded by the 
 examples, i.e. there is a set of attributes (one for each dimension of the multivariate 
 series) and a set of examples. Each example encodes the value vector for a new time point, 
 each attribute value represents another dimension of the multivariate series.
 <li>encode_series_by_attributes</li>: the series index variable (e.g. time) is encoded by 
 the attributes, i.e. there is a set of examples (one for each dimension of the multivariate 
 series) and a set of attributes. The set of attribute values for all examples encodes the 
 value vector for a new time point, each example represents another dimension of the 
 multivariate series.
 </ul>
 
 <p>Please note that the encoding as examples is usually more efficient with respect to the
 memory usage.</p>
#####
NoiseGenerator
This operator adds random attributes and white noise to the data. New random
 attributes are simply filled with random data which is not correlated to the
 label at all. Additionally, this operator might add noise to the label
 attribute or to the regular attributes. In case of a numerical label the
 given <code>label_noise</code> is the percentage of the label range which
 defines the standard deviation of normal distributed noise which is added to
 the label attribute. For nominal labels the parameter
 <code>label_noise</code> defines the probability to randomly change the
 nominal label value. In case of adding noise to regular attributes the
 parameter <code>default_attribute_noise</code> simply defines the standard
 deviation of normal distributed noise without using the attribute value
 range. Using the parameter list it is possible to set different noise levels
 for different attributes. However, it is not possible to add noise to nominal
 attributes.
#####
Nominal2Binary
This operator maps the values of all nominal values to binary attributes. For example,
 if a nominal attribute with name &quot;costs&quot; and possible nominal values
 &quot;low&quot;, &quot;moderate&quot;, and &quot;high&quot; is transformed, the result
 is a set of three binominal attributes &quot;costs = low&quot;, &quot;costs = moderate&quot;,
 and &quot;costs = high&quot;. Only one of the values of each attribute is true for a specific
 example, the other values are false.
#####
Nominal2Binominal
This operator maps the values of all nominal values to binary attributes. For example,
 if a nominal attribute with name &quot;costs&quot; and possible nominal values
 &quot;low&quot;, &quot;moderate&quot;, and &quot;high&quot; is transformed, the result
 is a set of three binominal attributes &quot;costs = low&quot;, &quot;costs = moderate&quot;,
 and &quot;costs = high&quot;. Only one of the values of each attribute is true for a specific
 example, the other values are false.
#####
Nominal2Numeric
This operator maps all non numeric attributes to real valued attributes.
 Nothing is done for numeric attributes, binary attributes are mapped to 0 and
 1.
 
 For nominal attributes one of the following calculations will be done:
 <ul>
 <li>Dichotomization, i.e. one new attribute for each value of the nominal
 attribute. The new attribute which corresponds to the actual nominal value
 gets value 1 and all other attributes gets value 0.</li>
 <li>Alternatively the values of nominal attributes can be seen as equally
 ranked, therefore the nominal attribute will simply be turned into a real
 valued attribute, the old values results in equidistant real values.</li>
 </ul>
 
 At this moment the same applies for ordinal attributes, in a future release
 more appropriate values based on the ranking between the ordinal values may
 be included.
#####
NominalNumbers2Numerical
<p>This operator transforms nominal attributes into numerical ones. In contrast to
 the NominalToNumeric operator, this operator directly parses numbers from
 the wrongly as nominal values encoded values. Please note that this operator
 will first check the stored nominal mappings for all attributes. If (old) mappings
 are still stored which actually are nominal (without the corresponding data being part of 
 the example set), the attribute will not be converted. Please use the operator
 <i>GuessValueTypes</i> in these cases.</p>
#####
Normalization
This operator performs a normalization. This can be done between a user
 defined minimum and maximum value or by a z-transformation, i.e. on mean 0
 and variance 1.
#####
Numeric2Binary
Converts all numerical attributes to binary ones. If the value of an
 attribute is between the specified minimal and maximal value, it becomes <em>false</em>, 
 otherwise <em>true</em>. If the value is missing, the new value will be missing. The default
 boundaries are both set to 0, thus only 0.0 is mapped to false and all other values are 
 mapped to true.
#####
Numeric2Binominal
Converts all numerical attributes to binary ones. If the value of an
 attribute is between the specified minimal and maximal value, it becomes <em>false</em>, 
 otherwise <em>true</em>. If the value is missing, the new value will be missing. The default
 boundaries are both set to 0, thus only 0.0 is mapped to false and all other values are 
 mapped to true.
#####
Numeric2Polynominal
Converts all numerical attributes to nominal ones. Each numerical value is simply
 used as nominal value of the new attribute. If the value is missing, the new value 
 will be missing. Please note that this operator might drastically increase memory
 usage if many different numerical values are used. Please use the available discretization
 operators then.
#####
Obfuscator
This operator takes an <code>ExampleSet</code> as input and maps all
 nominal values to randomly created strings. The names and the construction
 descriptions of all attributes will also replaced by random strings. This
 operator can be used to anonymize your data. It is possible to save the
 obfuscating map into a file which can be used to remap the old values and
 names. Please use the operator <code>Deobfuscator</code> for this purpose.
 The new example set can be written with an <code>ExampleSetWriter</code>.
#####
PCA
This operator performs a principal components analysis (PCA) using the
 covariance matrix. The user can specify the amount of variance to cover in
 the original data when retaining the best number of principal components. The
 user can also specify manually the number of principal components. The
 operator outputs a <code>PCAModel</code>. With the
 <code>ModelApplier</code> you can transform the features.
#####
PCAWeighting
Uses the factors of one of the principal components (default is the first) as
 feature weights. Please note that the PCA weighting operator is currently the only one 
 which also works on data sets without a label, i.e. for unsupervised learning.
#####
PSOWeighting
This operator performs the weighting of features with a particle swarm
 approach.
#####
Permutation
This operator creates a new, shuffled ExampleSet by making <em>a new copy</em>
  of the exampletable in main memory!
  Caution! System may run out of memory, if exampletable is too large.
#####
PrincipalComponentsGenerator
Builds the principal components of the given data. The user can specify the
 amount of variance to cover in the original data when retaining the best
 number of principal components. This operator makes use of the Weka
 implementation <code>PrincipalComponent</code>.
#####
RandomSelection
This operator selects a randomly chosen number of features randomly from the input example set.
 This can be useful in combination with a ParameterIteration operator or can be used
 as a baseline for significance test comparisons for feature selection techniques.
#####
Relief
<p>Relief measures the relevance of features by sampling examples 
 and comparing the value of the current feature for the nearest 
 example of the same and of a different class. This version also
 works for multiple classes and regression data sets. The resulting
 weights are normalized into the interval between 0 and 1.</p>
#####
RemoveCorrelatedFeatures
Removes (un-) correlated features due to the selected filter relation. The
 procedure is quadratic in number of attributes. In order to get more stable 
 results, the original, random, and reverse order of attributes is available.
 
 Please note that this operator might fail in some cases when the attributes
 should be filtered out, e.g. it might not be able to remove for example all 
 negative correlated features. The reason for this behaviour seems to be that 
 for the complete m x m - matrix of correlations (for m attributes) the 
 correlations will not be recalculated and hence not checked if one of the 
 attributes of the current pair was already marked for removal. That means 
 for three attributes a1, a2, and a3 that it might be that a2 was already ruled 
 out by the negative correlation with a1 and is now not able to rule out a3 any 
 longer.
#####
RemoveUselessAttributes
Removes useless attribute from the example set. Useless attributes are
 <ul>
 <li>nominal attributes which has the same value for more than <code>p</code>
 percent of all examples.</li>
 <li>numerical attributes which standard deviation is less or equal to a
 given deviation threshold <code>t</code>.</li>
 </ul>
#####
SOMDimensionalityReduction
This operator performs a dimensionality reduction based on a SOM (Self Organizing Map, aka Kohonen net).
#####
SVDReduction
A dimensionality reduction method based on Singular Value Decomposition. TODO: see super class
#####
SVMWeighting
Uses the coefficients of the normal vector of a linear SVM as feature weights.
 In contrast to most of the SVM based operators available in RapidMiner, this one works
 for multiple classes, too.
#####
Sampling
Simple sampling operator. This operator performs a random sampling of a given
 fraction. For example, if the input example set contains 5000 examples and
 the sample ratio is set to 0.1, the result will have approximately 500
 examples.
#####
Series2WindowExamples
<p>This operator transforms a given example set containing series data into a
 new example set containing single valued examples. For this purpose, windows with
 a specified window and step size are moved across the series and the series
 value lying horizon values after the window end is used as label which should
 be predicted. This operator can only be used for univariate series prediction. 
 For the multivariate case, please use the operator
 <i>MultivariateSeries2WindowExamples</i>.</p>
 
 <p>
 The series data must be given as ExampleSet. The parameter &quot;series_representation&quot;
 defines how the series data is represented by the ExampleSet:</p>
 <ul>
 <li>encode_series_by_examples</li>: the series index variable (e.g. time) is encoded by the examples, 
 i.e. there is a <em>single</em> attribute and a set of examples. Each example encodes the value for a new time point.
 <li>encode_series_by_attributes</li>: the series index variable (e.g. time) is encoded by the attributes, 
 i.e. there is a (set of) examples and a set of attributes. Each attribute value encodes the value for a 
 new time point. If there is more than one example, the windowing is performed for each example independently
 and all resulting window examples are merged into a complete example set.
 </ul>
 
 <p>Please note that the encoding as examples is usually more efficient with respect to the
 memory usage. To ensure backward compatibility, the default representation is, however, set 
 to time_as_attributes.</p>
#####
Single2Series
Transforms all regular attributes of a given example set into a value series.
 All attributes must have the same value type. Attributes with block type
 value series can be used by special feature extraction operators or by the
 operators from the value series plugin.
#####
SingleRuleWeighting
This operator calculates the relevance of a feature by computing the error
 rate of a OneR Model on the exampleSet without this feature.
#####
Sorting
<p>
 This operator sorts the given example set according to a single attribute. The example
 set is sorted according to the natural order of the values of this attribute either
 in increasing or in decreasing direction.
 </p>
#####
SplitSVMModel
Splits a <code>JMySVMModel</code> into an <code>ExampleSet</code> of the
 support vectors and <code>AttributeWeights</code> representing the normal
 of the hyperplane. Additionally the <code>ExampleSet</code> of the SVs
 contain a new attribute with the alpha-values.
#####
StandardDeviationWeighting
<p>
 Creates weights from the standard deviations of all attributes. The values can
 be normalized by the average, the minimum, or the maximum of the attribute.
 </p>
#####
StratifiedSampling
Stratified sampling operator. This operator performs a random sampling of a
 given fraction. In contrast to the simple sampling operator, this operator
 performs a stratified sampling for data sets with nominal label attributes,
 i.e. the class distributions remains (almost) the same after sampling. Hence,
 this operator cannot be applied on data sets without a label or with a
 numerical label. In these cases a simple sampling without stratification
 is performed.
#####
SymmetricalUncertaintyWeighting
<p>
 This operator calculates the relevance of an attribute by measuring 
 the symmetrical uncertainty with respect to the class. 
 The formulaization for this is:
 </p>
 
 <code>relevance = 2 * (P(Class) - P(Class | Attribute)) / P(Class) + P(Attribute)</code>
#####
TFIDFFilter
This operator generates TF-IDF values from the input data. The input example
 set must contain either simple counts, which will be normalized during
 calculation of the term frequency TF, or it already contains the calculated
 term frequency values (in this case no normalization will be done).
#####
UseRowAsAttributeNames
<p>This operators uses the values of the specified row of the data set as new
 attribute names (including both regular and special columns). This might
 be useful for example after a transpose operation. The row will be deleted 
 from the data set. Please note, however, that an internally used nominal 
 mapping will not be removed and following operators like <i>NominalNumbers2Numerical</i>
 could possibly not work as expected. In order to correct the value types and 
 nominal value mappings, one could use the operator <i>GuessValueTypes</i>
 after this operator.</p>
#####
UserBasedDiscretization
This operator discretizes a numerical attribute to either a nominal or an ordinal attribute. The numerical values are mapped to the classes according to the thresholds specified by the user. The user can define the classes by specifying the upper
 limits of each class. The lower limit of the next class is automatically specified as the upper limit of the previous one. A parameter defines to which adjacent class values that are equal to the given limits should be mapped. If the upper limit
 in the last list entry is not equal to Infinity, an additional class which is automatically named is added. If a '?' is given as class value the according numerical values are mapped to unknown values in the resulting attribute.
#####
W-ChiSquaredAttributeEval
Performs the AttributeEvaluator of Weka with the same name to determine a
 sort of attribute relevance. These relevance values build an instance of
 AttributeWeights. Therefore, they can be used by other operators which make
 use of such weights, like weight based selection or search heuristics which
 use attribute weights to speed up the search. See the Weka javadoc for
 further operator and parameter descriptions.
#####
W-CostSensitiveAttributeEval
Performs the AttributeEvaluator of Weka with the same name to determine a
 sort of attribute relevance. These relevance values build an instance of
 AttributeWeights. Therefore, they can be used by other operators which make
 use of such weights, like weight based selection or search heuristics which
 use attribute weights to speed up the search. See the Weka javadoc for
 further operator and parameter descriptions.
#####
W-GainRatioAttributeEval
Performs the AttributeEvaluator of Weka with the same name to determine a
 sort of attribute relevance. These relevance values build an instance of
 AttributeWeights. Therefore, they can be used by other operators which make
 use of such weights, like weight based selection or search heuristics which
 use attribute weights to speed up the search. See the Weka javadoc for
 further operator and parameter descriptions.
#####
W-InfoGainAttributeEval
Performs the AttributeEvaluator of Weka with the same name to determine a
 sort of attribute relevance. These relevance values build an instance of
 AttributeWeights. Therefore, they can be used by other operators which make
 use of such weights, like weight based selection or search heuristics which
 use attribute weights to speed up the search. See the Weka javadoc for
 further operator and parameter descriptions.
#####
W-OneRAttributeEval
Performs the AttributeEvaluator of Weka with the same name to determine a
 sort of attribute relevance. These relevance values build an instance of
 AttributeWeights. Therefore, they can be used by other operators which make
 use of such weights, like weight based selection or search heuristics which
 use attribute weights to speed up the search. See the Weka javadoc for
 further operator and parameter descriptions.
#####
W-PrincipalComponents
Performs the AttributeEvaluator of Weka with the same name to determine a
 sort of attribute relevance. These relevance values build an instance of
 AttributeWeights. Therefore, they can be used by other operators which make
 use of such weights, like weight based selection or search heuristics which
 use attribute weights to speed up the search. See the Weka javadoc for
 further operator and parameter descriptions.
#####
W-ReliefFAttributeEval
Performs the AttributeEvaluator of Weka with the same name to determine a
 sort of attribute relevance. These relevance values build an instance of
 AttributeWeights. Therefore, they can be used by other operators which make
 use of such weights, like weight based selection or search heuristics which
 use attribute weights to speed up the search. See the Weka javadoc for
 further operator and parameter descriptions.
#####
W-SVMAttributeEval
Performs the AttributeEvaluator of Weka with the same name to determine a
 sort of attribute relevance. These relevance values build an instance of
 AttributeWeights. Therefore, they can be used by other operators which make
 use of such weights, like weight based selection or search heuristics which
 use attribute weights to speed up the search. See the Weka javadoc for
 further operator and parameter descriptions.
#####
W-SymmetricalUncertAttributeEval
Performs the AttributeEvaluator of Weka with the same name to determine a
 sort of attribute relevance. These relevance values build an instance of
 AttributeWeights. Therefore, they can be used by other operators which make
 use of such weights, like weight based selection or search heuristics which
 use attribute weights to speed up the search. See the Weka javadoc for
 further operator and parameter descriptions.
#####
WeightGuidedFeatureSelection
<p>
 This operator uses input attribute weights to determine the order of features
 added to the feature set starting with the feature set containing only the
 feature with highest weight. The inner operators must provide a performance
 vector to determine the fitness of the current feature set, e.g. a cross
 validation of a learning scheme for a wrapper evaluation. Stops if adding the
 last <code>k</code> features does not increase the performance or if all
 features were added. The value of <code>k</code> can be set with the
 parameter <code>generations_without_improval</code>.
 </p>
#####
WeightOptimization
Performs a feature selection guided by the AttributeWeights. Forward
 selection means that features with the highest weight-value are selected
 first (starting with an empty selection). Backward elemination means that
 features with the smallest weight value are eleminated first (starting with
 the full feature set).
#####
WeightedBootstrapping
This operator constructs a bootstrapped sample from the given example set which must provide 
 a weight attribute. If no weight attribute was provided this operator will stop the process
 with an error message. See the operator <i>Bootstrapping</i> for more information.
#####
YAGGA
YAGGA is an acronym for Yet Another Generating Genetic Algorithm. Its
 approach to generating new attributes differs from the original one. The
 (generating) mutation can do one of the following things with different
 probabilities:
 <ul>
 <li>Probability <i>p/4</i>: Add a newly generated attribute to the
 feature vector</li>
 <li>Probability <i>p/4</i>: Add a randomly chosen original attribute
 to the feature vector</li>
 <li>Probability <i>p/2</i>: Remove a randomly chosen attribute from
 the feature vector</li>
 </ul>
 Thus it is guaranteed that the length of the feature vector can both grow and
 shrink. On average it will keep its original length, unless longer or shorter
 individuals prove to have a better fitness.
 
 Since this operator does not contain algorithms to extract features from
 value series, it is restricted to example sets with only single attributes.
 For (automatic) feature extraction from values series the value series plugin
 for RapidMiner written by Ingo Mierswa should be used. It is available at <a
 href="http://rapid-i.com">http://rapid-i.com</a>.
#####
YAGGA2
<p>YAGGA is an acronym for Yet Another Generating Genetic Algorithm. Its
 approach to generating new attributes differs from the original one. The
 (generating) mutation can do one of the following things with different
 probabilities:</p>
 <ul>
 <li>Probability <i>p/4</i>: Add a newly generated attribute to the
 feature vector</li>
 <li>Probability <i>p/4</i>: Add a randomly chosen original attribute
 to the feature vector</li>
 <li>Probability <i>p/2</i>: Remove a randomly chosen attribute from
 the feature vector</li>
 </ul>
 <p>Thus it is guaranteed that the length of the feature vector can both grow and
 shrink. On average it will keep its original length, unless longer or shorter
 individuals prove to have a better fitness.</p>
 
 <p>In addition to the usual YAGGA operator, this operator allows more feature
 generators and provides several techniques for intron prevention. This leads to 
 smaller example sets containing less redundant features.</p>
 
 <p>Since this operator does not contain algorithms to extract features from
 value series, it is restricted to example sets with only single attributes.
 For (automatic) feature extraction from values series the value series plugin
 for RapidMiner should be used.</p>

 <p>For more information please refer to</p>
 <p>Mierswa, Ingo (2007): <em>RobustGP: Intron-Free Multi-Objective Feature Construction</em> (to appear)</p>
#####

Anova
Determines if the null hypothesis (all actual mean values are the same) holds
 for the input performance vectors. This operator uses an ANalysis Of
 VAriances approach to determine probability that the null hypothesis is
 wrong.
#####
AttributeCounter
Returns a performance vector just counting the number of attributes currently
 used for the given example set.
#####
BatchSlidingWindowValidation
<p>
 The <code>BatchSlidingWindowValidation</code> is similar to the usual 
 <i>SlidingWindowValidation</i>. This operator, however, does not 
 split the data itself in windows of predefined widths but uses the partition
 defined by the special attribute &quot;batch&quot;. This can be an arbitrary
 nominal or integer attribute where each possible value occurs at least once
 (since many learning schemes depend on this minimum number of examples).
 In each iteration, the next training batch is used for learning and the batch
 after this for prediction. It is also possible to perform a cumulative batch
 creation where each test batch will simply be added to the current training
 batch for the training in the next generation. 
 </p> 
 
 <p>
 The first inner operator must accept an
 <i>ExampleSet</i> while the second must accept an
 <i>ExampleSet</i> and the output of the first (which
 is in most cases a <i>Model</i>) and must produce
 a <i>PerformanceVector</i>.
 </p>
#####
BatchXValidation
<p>
 <code>BatchXValidation</code> encapsulates a cross-validation process. The
 example set <i>S</i> is split up into <var> number_of_validations</var>
 subsets <i>S_i</i>. The inner operators are applied
 <var>number_of_validations</var> times using <i>S_i</i> as the test
 set (input of the second inner operator) and <i>Sbackslash S_i</i>
 training set (input of the first inner operator).
 </p>
 
 <p>In contrast to the usual cross validation operator (see <i>XValidation</i>)
 this operator does not (randomly) split the data itself but uses the partition
 defined by the special attribute &quot;batch&quot;. This can be an arbitrary
 nominal or integer attribute where each possible value occurs at least once
 (since many learning schemes depend on this minimum number of examples). 
 </p> 
 
 <p>
 The first inner operator must accept an
 <i>ExampleSet</i> while the second must accept an
 <i>ExampleSet</i> and the output of the first (which
 is in most cases a <i>Model</i>) and must produce
 a <i>PerformanceVector</i>.
 </p>
#####
BinominalClassificationPerformance
<p>This performance evaluator operator should be used for classification tasks, 
 i.e. in cases where the label attribute has a binominal value type.
 Other polynominal classification tasks, i.e. tasks with more than two classes
 can be handled by the <i>PolynominalClassificationPerformanceEvaluator</i> operator.
 This operator expects a test <i>ExampleSet</i>
 as input, whose elements have both true and predicted labels, and delivers as
 output a list of performance values according to a list of performance
 criteria that it calculates. If an input performance vector was already
 given, this is used for keeping the performance values.</p> 
 
 <p>All of the performance criteria can be switched on using boolean parameters. 
 Their values can be queried by a ProcessLogOperator using the same names.
 The main criterion is used for comparisons and need to be specified only for
 processes where performance vectors are compared, e.g. feature selection
 or other meta optimization process setups. 
 If no other main criterion was selected, the first criterion in the 
 resulting performance vector will be assumed to be the main criterion.</p> 
 
 <p>The resulting performance vectors are usually compared with a standard
 performance comparator which only compares the fitness values of the main
 criterion. Other implementations than this simple comparator can be
 specified using the parameter <var>comparator_class</var>. This may for
 instance be useful if you want to compare performance vectors according to
 the weighted sum of the individual criteria. In order to implement your own
 comparator, simply subclass <i>PerformanceComparator</i>. Please note that
 for true multi-objective optimization usually another selection scheme is
 used instead of simply replacing the performance comparator.</p>
#####
BootstrappingValidation
<p>This validation operator performs several bootstrapped samplings (sampling with replacement)
 on the input set and trains a model on these samples. The remaining samples, i.e. those which
 were not sampled, build a test set on which the model is evaluated. This process is repeated
 for the specified number of iterations after which the average performance is calculated.</p>
 
 <p>The basic setup is the same as for the usual cross validation operator. The first inner 
 operator must provide a model and the second a performance vector. Please note that this operator
 does not regard example weights, i.e. weights specified in a weight column.</p>
#####
CFSFeatureSetEvaluator
<p>
 CFS attribute subset evaluator. For more information see: <br/> Hall, M. A.
 (1998). Correlation-based Feature Subset Selection for Machine Learning.
 Thesis submitted in partial fulfilment of the requirements of the degree of
 Doctor of Philosophy at the University of Waikato.
 </p>
 
 <p>
 This operator creates a filter based performance measure for a feature
 subset. It evaluates the worth of a subset of attributes by considering the
 individual predictive ability of each feature along with the degree of
 redundancy between them. Subsets of features that are highly correlated with
 the class while having low intercorrelation are preferred.
 </p>
 
 <p>
 This operator can be applied on both numerical and nominal data sets.
 </p>
#####
ClassificationPerformance
<p>This performance evaluator operator should be used for classification tasks, 
 i.e. in cases where the label attribute has a (poly-)nominal value type.
 The operator expects a test <i>ExampleSet</i>
 as input, whose elements have both true and predicted labels, and delivers as
 output a list of performance values according to a list of performance
 criteria that it calculates. If an input performance vector was already
 given, this is used for keeping the performance values.</p> 
 
 <p>All of the performance criteria can be switched on using boolean parameters. 
 Their values can be queried by a ProcessLogOperator using the same names.
 The main criterion is used for comparisons and need to be specified only for
 processes where performance vectors are compared, e.g. feature selection
 or other meta optimization process setups. 
 If no other main criterion was selected, the first criterion in the 
 resulting performance vector will be assumed to be the main criterion.</p> 
 
 <p>The resulting performance vectors are usually compared with a standard
 performance comparator which only compares the fitness values of the main
 criterion. Other implementations than this simple comparator can be
 specified using the parameter <var>comparator_class</var>. This may for
 instance be useful if you want to compare performance vectors according to
 the weighted sum of the individual criteria. In order to implement your own
 comparator, simply subclass <i>PerformanceComparator</i>. Please note that
 for true multi-objective optimization usually another selection scheme is
 used instead of simply replacing the performance comparator.</p>
#####
ClusterCentroidEvaluator
An evaluator for centroid based clustering methods.
#####
ClusterDensityEvaluator
This operator is used to evaluate a flat cluster model based on diverse density measures. Currently, only the avg. within cluster similarity/distance (depending on the type of SimilarityMeasure input object used) is supported.
#####
ClusterModelFScore
Compares two cluster models by searching for each concept a best matching one in the compared cluster model in terms of f-measure. The average f-measure of the best matches is then the overall cluster model similarity.
#####
ClusterModelLabelComparator
Compares two hierarchical clustering models according to the label of their root node. If this label is equal, 1 is returned, 0 otherwise.
#####
ClusterNumberEvaluator
This operator does actually not compute a performance criterion but simply provides the number of cluster as a value.
#####
ConsistencyFeatureSetEvaluator
<p>
 Consistency attribute subset evaluator. For more information see: <br/> Liu,
 H., and Setiono, R., (1996). A probabilistic approach to feature selection -
 A filter solution. In 13th International Conference on Machine Learning
 (ICML'96), July 1996, pp. 319-327. Bari, Italy.
 </p>
 
 <p>
 This operator evaluates the worth of a subset of attributes by the level of
 consistency in the class values when the training instances are projected
 onto the subset of attributes. Consistency of any subset can never be lower
 than that of the full set of attributes, hence the usual practice is to use
 this subset evaluator in conjunction with a Random or Exhaustive search which
 looks for the smallest subset with consistency equal to that of the full set
 of attributes.
 </p>
 
 <p>
 This operator can only be applied for classification data sets, i.e. where
 the label attribute is nominal.
 </p>
#####
ConstraintClusterValidation
Evaluates a ClusterModel with regard to a given ClusterConstraintList and
 takes the weight of the violated constraints as performance value.
#####
CostEvaluator
This operator provides the ability to evaluate classification costs.
 Therefore a cost matrix might be specified, denoting the costs for every
 possible classification outcome: predicted label x real label.
 Costs will be minimized during optimization.
#####
FixedSplitValidation
A FixedSplitValidationChain splits up the example set at a fixed point into a
 training and test set and evaluates the model (linear sampling). For
 non-linear sampling methods, i.e. the data is shuffled, the specified amounts
 of data are used as training and test set. The sum of both must be smaller
 than the input example set size. <br/>
 
 At least either the training set size must be specified (rest is used for
 testing) or the test set size must be specified (rest is used for training).
 If both are specified, the rest is not used at all.
 
 The first inner operator must accept an
 <i>ExampleSet</i> while the second must accept an
 <i>ExampleSet</i> and the output of the first (which
 in most cases is a <i>Model</i>) and must produce
 a <i>PerformanceVector</i>.
#####
ItemDistributionEvaluator
Evaluates flat cluster models on how well the items are distributed over the clusters.
#####
IteratingPerformanceAverage
This operator chain performs the inner operators the given number of times.
 The inner operators must provide a PerformanceVector. These are averaged and
 returned as result.
#####
MinMaxWrapper
Wraps a <i>MinMaxCriterion</i> around each performance criterion of type
 MeasuredPerformance. This criterion uses the minimum fitness achieved instead
 of the average fitness or arbitrary weightings of both. Please note that the
 average values stay the same and only the fitness values change.
#####
Performance
<p>In contrast to the other performance evaluation methods, this performance 
 evaluator operator can be used for all types of learning tasks. It will 
 automatically determine the learning task type and will calculate the most
 common criteria for this type. For more sophisticated performance calculations,
 you should check the operators <i>RegressionPerformanceEvaluator</i>,
 <i>PolynominalClassificationPerformanceEvaluator</i>, or
 <i>BinominalClassificationPerformanceEvaluator</i>. You can even
 simply write your own performance measure and calculate it with the
 operator <i>UserBasedPerformanceEvaluator</i>.</p>
 
 <p>The operator expects a test <i>ExampleSet</i>
 as input, whose elements have both true and predicted labels, and delivers as
 output a list of most commmon performance values for the provided learning
 task type (regression of (binominal) classification. If an input performance 
 vector was already given, this is used for keeping the performance values.</p>
#####
PerformanceEvaluator
<p>A performance evaluator is an operator that expects a test <i>ExampleSet</i>
 as input, whose elements have both true and predicted labels, and delivers as
 output a list of performance values according to a list of performance
 criteria that it calculates. If an input performance vector was already
 given, this is used for keeping the performance values.</p> 
 
 <p>All of the performance criteria can be switched on using boolean parameters. 
 Their values can be queried by a ProcessLogOperator using the same names.
 The main criterion is used for comparisons and need to be specified only for
 processes where performance vectors are compared, e.g. feature selection
 processes. If no other main criterion was selected the first criterion in the 
 resulting performance vector will be assumed to be the main criterion.</p> 
 
 <p>The resulting performance vectors are usually compared with a standard
 performance comparator which only compares the fitness values of the main
 criterion. Other implementations than this simple comparator can be
 specified using the parameter <var>comparator_class</var>. This may for
 instance be useful if you want to compare performance vectors according to
 the weighted sum of the individual criteria. In order to implement your own
 comparator, simply subclass <i>PerformanceComparator</i>. Please note that
 for true multi-objective optimization usually another selection scheme is
 used instead of simply replacing the performance comparator.</p>
 
 <p>Additional user-defined implementations of <i>PerformanceCriterion</i> 
 can be specified by using the parameter list
 <var>additional_performance_criteria</var>. Each key/value pair in this list
 must specify a fully qualified classname (as the key), and a string
 parameter (as value) that is passed to the constructor. Please make sure
 that the class files are in the classpath (this is the case if the
 implementations are supplied by a plugin) and that they implement a
 one-argument constructor taking a string parameter. It must also be ensured
 that these classes extend <i>MeasuredPerformance</i> since the PerformanceEvaluator
 operator will only support these criteria. Please note that only the
 first three user defined criteria can be used as logging value with names
 &quot;user1&quot;, ... , &quot;user3&quot;.</p>
#####
RegressionPerformance
<p>This performance evaluator operator should be used for regression tasks, 
 i.e. in cases where the label attribute has a numerical value type.
 The operator expects a test <i>ExampleSet</i>
 as input, whose elements have both true and predicted labels, and delivers as
 output a list of performance values according to a list of performance
 criteria that it calculates. If an input performance vector was already
 given, this is used for keeping the performance values.</p> 
 
 <p>All of the performance criteria can be switched on using boolean parameters. 
 Their values can be queried by a ProcessLogOperator using the same names.
 The main criterion is used for comparisons and need to be specified only for
 processes where performance vectors are compared, e.g. feature selection
 or other meta optimization process setups. 
 If no other main criterion was selected, the first criterion in the 
 resulting performance vector will be assumed to be the main criterion.</p> 
 
 <p>The resulting performance vectors are usually compared with a standard
 performance comparator which only compares the fitness values of the main
 criterion. Other implementations than this simple comparator can be
 specified using the parameter <var>comparator_class</var>. This may for
 instance be useful if you want to compare performance vectors according to
 the weighted sum of the individual criteria. In order to implement your own
 comparator, simply subclass <i>PerformanceComparator</i>. Please note that
 for true multi-objective optimization usually another selection scheme is
 used instead of simply replacing the performance comparator.</p>
#####
SimpleValidation
A <code>RandomSplitValidationChain</code> splits up the example set into a
 training and test set and evaluates the model. The first inner operator must
 accept an <i>ExampleSet</i> while the second must
 accept an <i>ExampleSet</i> and the output of the
 first (which is in most cases a <i>Model</i>) and
 must produce a <i>PerformanceVector</i>.
#####
SimpleWrapperValidation
This operator evaluates the performance of feature weighting algorithms
 including feature selection. The first inner operator is the weighting
 algorithm to be evaluated itself. It must return an attribute weights vector
 which is applied on the data. Then a new model is created using the second
 inner operator and a performance is retrieved using the third inner operator.
 This performance vector serves as a performance indicator for the actual
 algorithm.
 
 This implementation is described for the <i>RandomSplitValidationChain</i>.
#####
SlidingWindowValidation
This is a special validation chain which can only be used for series predictions where 
 the time points are encoded as examples. It uses a certain window of examples for
 training and uses another window (after horizon examples, i.e. time points) for testing.
 The window is moved across the example set and all performance measurements are 
 averaged afterwards. The parameter &quot;cumulative_training&quot; indicates if all
 former examples should be used for training (instead of only the current window).
#####
T-Test
Determines if the null hypothesis (all actual mean values are the same) holds
 for the input performance vectors. This operator uses a simple (pairwise)
 t-test to determine the probability that the null hypothesis is wrong. Since
 a t-test can only be applied on two performance vectors this test will be
 applied to all possible pairs. The result is a significance matrix. However,
 pairwise t-test may introduce a larger type I error. It is recommended to
 apply an additional ANOVA test to determine if the null hypothesis is wrong
 at all.
#####
UserBasedPerformance
<p>This performance evaluator operator should be used for regression tasks, 
 i.e. in cases where the label attribute has a numerical value type.
 The operator expects a test <i>ExampleSet</i>
 as input, whose elements have both true and predicted labels, and delivers as
 output a list of performance values according to a list of performance
 criteria that it calculates. If an input performance vector was already
 given, this is used for keeping the performance values.</p> 
 
 <p>Additional user-defined implementations of <i>PerformanceCriterion</i> 
 can be specified by using the parameter list
 <var>additional_performance_criteria</var>. Each key/value pair in this list
 must specify a fully qualified classname (as the key), and a string
 parameter (as value) that is passed to the constructor. Please make sure
 that the class files are in the classpath (this is the case if the
 implementations are supplied by a plugin) and that they implement a
 one-argument constructor taking a string parameter. It must also be ensured
 that these classes extend <i>MeasuredPerformance</i> since the PerformanceEvaluator
 operator will only support these criteria. Please note that only the
 first three user defined criteria can be used as logging value with names
 &quot;user1&quot;, ... , &quot;user3&quot;.</p> 
 
 <p>The resulting performance vectors are usually compared with a standard
 performance comparator which only compares the fitness values of the main
 criterion. Other implementations than this simple comparator can be
 specified using the parameter <var>comparator_class</var>. This may for
 instance be useful if you want to compare performance vectors according to
 the weighted sum of the individual criteria. In order to implement your own
 comparator, simply subclass <i>PerformanceComparator</i>. Please note that
 for true multi-objective optimization usually another selection scheme is
 used instead of simply replacing the performance comparator.</p>
#####
WeightedBootstrappingValidation
<p>This validation operator performs several bootstrapped samplings (sampling with replacement)
 on the input set and trains a model on these samples. The remaining samples, i.e. those which
 were not sampled, build a test set on which the model is evaluated. This process is repeated
 for the specified number of iterations after which the average performance is calculated.</p>
 
 <p>The basic setup is the same as for the usual cross validation operator. The first inner 
 operator must provide a model and the second a performance vector. Please note that this operator
 does not regard example weights, i.e. weights specified in a weight column.</p>
#####
WeightedPerformanceCreator
Returns a performance vector containing the weighted fitness value of the
 input criteria.
#####
WrapperXValidation
This operator evaluates the performance of feature weighting and selection
 algorithms. The first inner operator is the algorithm to be evaluated itself.
 It must return an attribute weights vector which is applied on the test data.
 This fold is used to create a new model using the second inner operator and
 retrieve a performance vector using the third inner operator. This
 performance vector serves as a performance indicator for the actual
 algorithm. This implementation of a MethodValidationChain works similar to
 the <i>XValidation</i>.
#####
XValidation
<p>
 <code>XValidation</code> encapsulates a cross-validation process. The
 example set <i>S</i> is split up into <var> number_of_validations</var>
 subsets <i>S_i</i>. The inner operators are applied
 <var>number_of_validations</var> times using <i>S_i</i> as the test
 set (input of the second inner operator) and <i>Sbackslash S_i</i>
 training set (input of the first inner operator).
 </p>
 
 <p>
 The first inner operator must accept an
 <i>ExampleSet</i> while the second must accept an
 <i>ExampleSet</i> and the output of the first (which
 is in most cases a <i>Model</i>) and must produce
 a <i>PerformanceVector</i>.
 </p>
 
 <p>
 Like other validation schemes the RapidMiner cross validation can use several types
 of sampling for building the subsets. Linear sampling simply divides the
 example set into partitions without changing the order of the examples.
 Shuffled sampling build random subsets from the data. Stratifed sampling
 builds random subsets and ensures that the class distribution in the subsets
 is the same as in the whole example set.
 </p>
#####

CorrelationMatrix
<p>This operator calculates the correlation matrix between all attributes of the
 input example set. Furthermore, attribute weights based on the correlations
 can be returned. This allows the deselection of highly correlated attributes
 with the help of an
 <i>AttributeWeightSelection</i>
 operator. If no weights should be created, this operator produces simply a
 correlation matrix which up to now cannot be used by other operators but can
 be displayed to the user in the result tab.</p> 
 
 <p>Please note that this simple implementation
 performs a data scan for each attribute combination and might therefore take
 some time for non-memory example tables.</p>
#####
CovarianceMatrix
This operator calculates the covariances between all attributes
  of the input example set and returns a covariance matrix object
  which can be visualized.
#####
DataStatistics
This operators calculates some very simple statistics about the given example
 set. These are the ranges of the attributes and the average or mode values
 for numerical or nominal attributes respectively. These informations are
 automatically calculated and displayed by the graphical user interface of
 RapidMiner. Since they cannot be displayed with the command line version of RapidMiner
 this operator can be used as a workaround in cases where the graphical user
 interface cannot be used.
#####
ExampleVisualizer
Remembers the given example set and uses the ids provided by this set
 for the query for the corresponding example and the creation of a
 generic example visualizer. This visualizer simply displays the attribute
 values of the example. Adding this operator is often necessary to 
 enable the visualization of single examples in the provided plotter
 components.
#####
ExperimentLog
This operator records almost arbitrary data. It can written to a file which
 can be read e.g. by gnuplot. Alternatively, the collected data can be plotted
 by the GUI. This is even possible during process runtime (i.e. online
 plotting).<br/>
 
 Parameters in the list <code>log</code> are interpreted as follows: The
 <var>key</var> gives the name for the column name (e.g. for use in the
 plotter). The <var>value</var> specifies where to retrieve the value from.
 This is best explained by an example:
 <ul>
 <li>If the value is <code>operator.Evaluator.value.absolute</code>, the
 ProcessLogOperator looks up the operator with the name
 <code>Evaluator</code>. If this operator is a
 <i>PerformanceEvaluator</i>, it has a
 value named <var>absolute</var> which gives the absolute error of the last
 evaluation. This value is queried by the ProcessLogOperator</li>
 <li>If the value is <code>operator.SVMLearner.parameter.C</code>, the
 ProcessLogOperator looks up the parameter <var>C</var> of the operator
 named <code>SVMLearner</code>.</li>
 </ul>
 Each time the ProcessLogOperator is applied, all the values and parameters
 specified by the list <var>log</var> are collected and stored in a data row.
 When the process finishes, the operator writes the collected data rows to
 a file (if specified). In GUI mode, 2D or 3D plots are automatically
 generated and displayed in the result viewer. <br/> Please refer to section
 <i>Advanced Processes/Parameter and performance analysis</i>
 for an example application.
#####
LiftChart
This operator creates a Lift chart for the given example set and model. The model
 will be applied on the example set and a lift chart will be produced afterwards.
 
 Please note that a predicted label of the given example set will be removed during 
 the application of this operator.
#####
ModelVisualizer
This class provides an operator for the visualization of arbitrary models with
 help of the dimensionality reduction via a SOM of both the data set and the 
 given model.
#####
ProcessLog
This operator records almost arbitrary data. It can written to a file which
 can be read e.g. by gnuplot. Alternatively, the collected data can be plotted
 by the GUI. This is even possible during process runtime (i.e. online
 plotting).<br/>
 
 Parameters in the list <code>log</code> are interpreted as follows: The
 <var>key</var> gives the name for the column name (e.g. for use in the
 plotter). The <var>value</var> specifies where to retrieve the value from.
 This is best explained by an example:
 <ul>
 <li>If the value is <code>operator.Evaluator.value.absolute</code>, the
 ProcessLogOperator looks up the operator with the name
 <code>Evaluator</code>. If this operator is a
 <i>PerformanceEvaluator</i>, it has a
 value named <var>absolute</var> which gives the absolute error of the last
 evaluation. This value is queried by the ProcessLogOperator</li>
 <li>If the value is <code>operator.SVMLearner.parameter.C</code>, the
 ProcessLogOperator looks up the parameter <var>C</var> of the operator
 named <code>SVMLearner</code>.</li>
 </ul>
 Each time the ProcessLogOperator is applied, all the values and parameters
 specified by the list <var>log</var> are collected and stored in a data row.
 When the process finishes, the operator writes the collected data rows to
 a file (if specified). In GUI mode, 2D or 3D plots are automatically
 generated and displayed in the result viewer. <br/> Please refer to section
 <i>Advanced Processes/Parameter and performance analysis</i>
 for an example application.
#####
ROCChart
This operator creates a ROC chart for the given example set and model. The model
 will be applied on the example set and a ROC chart will be produced afterwards. If
 you are interested in finding an optimal threshold, the operator 
 <i>ThresholdFinder</i> should be used. If
 you are interested in the performance criterion Area-Under-Curve (AUC) the usual
 <i>PerformanceEvaluator</i> can be used. This operator just presents a ROC plot
 for a given model and data set.
 
 Please note that a predicted label of the given example set will be removed during 
 the application of this operator.
#####
ROCComparator
This operator uses its inner operators (each of those must produce a model) and
 calculates the ROC curve for each of them. All ROC curves together are 
 plotted in the same plotter. The comparison is based on the average values of a 
 k-fold cross validation. Alternatively, this operator can use an internal split
 into a test and a training set from the given data set.
 
 Please note that a former predicted label of the given example set will be removed during 
 the application of this operator.
#####
TransitionMatrix
Creates a transition matrix for a given nominal attribute. An entry v(x,y) in the matrix
 denotes the conditional probability that value y occurs, after value x occurred.
#####


